{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84705,"databundleVersionId":9755748,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom PIL import Image\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-20T06:56:36.973563Z","iopub.execute_input":"2024-10-20T06:56:36.974293Z","iopub.status.idle":"2024-10-20T06:56:42.220057Z","shell.execute_reply.started":"2024-10-20T06:56:36.974250Z","shell.execute_reply":"2024-10-20T06:56:42.219051Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import warnings\n\n# Suppress all warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T06:56:42.221830Z","iopub.execute_input":"2024-10-20T06:56:42.222278Z","iopub.status.idle":"2024-10-20T06:56:42.226561Z","shell.execute_reply.started":"2024-10-20T06:56:42.222243Z","shell.execute_reply":"2024-10-20T06:56:42.225634Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/visual-taxonomy/train.csv')\ndf_test = pd.read_csv('/kaggle/input/visual-taxonomy/test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T06:56:42.227836Z","iopub.execute_input":"2024-10-20T06:56:42.228131Z","iopub.status.idle":"2024-10-20T06:56:42.484889Z","shell.execute_reply.started":"2024-10-20T06:56:42.228093Z","shell.execute_reply":"2024-10-20T06:56:42.483905Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df_train = df_train[:1000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T06:56:42.516188Z","iopub.execute_input":"2024-10-20T06:56:42.516598Z","iopub.status.idle":"2024-10-20T06:56:42.520942Z","shell.execute_reply.started":"2024-10-20T06:56:42.516546Z","shell.execute_reply":"2024-10-20T06:56:42.520023Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"<h1>For Women Tops & Tunics</h1>","metadata":{}},{"cell_type":"code","source":"df_train = df_train[df_train['Category'] == 'Women Tops & Tunics']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T06:56:42.487487Z","iopub.execute_input":"2024-10-20T06:56:42.488256Z","iopub.status.idle":"2024-10-20T06:56:42.515094Z","shell.execute_reply.started":"2024-10-20T06:56:42.488210Z","shell.execute_reply":"2024-10-20T06:56:42.514401Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df_train.drop(columns=['Category'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T06:56:42.522545Z","iopub.execute_input":"2024-10-20T06:56:42.523536Z","iopub.status.idle":"2024-10-20T06:56:42.549350Z","shell.execute_reply.started":"2024-10-20T06:56:42.523492Z","shell.execute_reply":"2024-10-20T06:56:42.548522Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"          id  len     attr_1   attr_2   attr_3       attr_4  attr_5   attr_6  \\\n51209  51375   10      black  regular      NaN          NaN     NaN      NaN   \n51210  51376   10  navy blue   fitted     crop         high  casual  default   \n51211  51377   10        red  regular  regular   round neck  casual  printed   \n51212  51378   10    default   fitted     crop     stylised  casual    solid   \n51213  51379   10    default     boxy  regular   round neck  casual  printed   \n...      ...  ...        ...      ...      ...          ...     ...      ...   \n52204  52370   10        red  default     crop   round neck     NaN  printed   \n52205  52371   10      white  regular  regular   round neck  casual  printed   \n52206  52372   10        NaN      NaN      NaN  square neck     NaN      NaN   \n52207  52373   10      peach   fitted  regular   round neck  casual    solid   \n52208  52374   10       pink   fitted  regular   round neck  casual    solid   \n\n           attr_7                 attr_8           attr_9  attr_10  \n51209         NaN                    NaN  regular sleeves      NaN  \n51210       solid          short sleeves          default  knitted  \n51211  typography             sleeveless       sleeveless      NaN  \n51212       solid          short sleeves  regular sleeves  default  \n51213  typography          short sleeves          default      NaN  \n...           ...                    ...              ...      ...  \n52204         NaN             sleeveless       sleeveless      NaN  \n52205     graphic          short sleeves  regular sleeves  tie-ups  \n52206         NaN          short sleeves              NaN      NaN  \n52207       solid  three-quarter sleeves              NaN  knitted  \n52208       solid          short sleeves              NaN  knitted  \n\n[1000 rows x 12 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>len</th>\n      <th>attr_1</th>\n      <th>attr_2</th>\n      <th>attr_3</th>\n      <th>attr_4</th>\n      <th>attr_5</th>\n      <th>attr_6</th>\n      <th>attr_7</th>\n      <th>attr_8</th>\n      <th>attr_9</th>\n      <th>attr_10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>51209</th>\n      <td>51375</td>\n      <td>10</td>\n      <td>black</td>\n      <td>regular</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>regular sleeves</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>51210</th>\n      <td>51376</td>\n      <td>10</td>\n      <td>navy blue</td>\n      <td>fitted</td>\n      <td>crop</td>\n      <td>high</td>\n      <td>casual</td>\n      <td>default</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>default</td>\n      <td>knitted</td>\n    </tr>\n    <tr>\n      <th>51211</th>\n      <td>51377</td>\n      <td>10</td>\n      <td>red</td>\n      <td>regular</td>\n      <td>regular</td>\n      <td>round neck</td>\n      <td>casual</td>\n      <td>printed</td>\n      <td>typography</td>\n      <td>sleeveless</td>\n      <td>sleeveless</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>51212</th>\n      <td>51378</td>\n      <td>10</td>\n      <td>default</td>\n      <td>fitted</td>\n      <td>crop</td>\n      <td>stylised</td>\n      <td>casual</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>regular sleeves</td>\n      <td>default</td>\n    </tr>\n    <tr>\n      <th>51213</th>\n      <td>51379</td>\n      <td>10</td>\n      <td>default</td>\n      <td>boxy</td>\n      <td>regular</td>\n      <td>round neck</td>\n      <td>casual</td>\n      <td>printed</td>\n      <td>typography</td>\n      <td>short sleeves</td>\n      <td>default</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>52204</th>\n      <td>52370</td>\n      <td>10</td>\n      <td>red</td>\n      <td>default</td>\n      <td>crop</td>\n      <td>round neck</td>\n      <td>NaN</td>\n      <td>printed</td>\n      <td>NaN</td>\n      <td>sleeveless</td>\n      <td>sleeveless</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>52205</th>\n      <td>52371</td>\n      <td>10</td>\n      <td>white</td>\n      <td>regular</td>\n      <td>regular</td>\n      <td>round neck</td>\n      <td>casual</td>\n      <td>printed</td>\n      <td>graphic</td>\n      <td>short sleeves</td>\n      <td>regular sleeves</td>\n      <td>tie-ups</td>\n    </tr>\n    <tr>\n      <th>52206</th>\n      <td>52372</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>square neck</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>short sleeves</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>52207</th>\n      <td>52373</td>\n      <td>10</td>\n      <td>peach</td>\n      <td>fitted</td>\n      <td>regular</td>\n      <td>round neck</td>\n      <td>casual</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>three-quarter sleeves</td>\n      <td>NaN</td>\n      <td>knitted</td>\n    </tr>\n    <tr>\n      <th>52208</th>\n      <td>52374</td>\n      <td>10</td>\n      <td>pink</td>\n      <td>fitted</td>\n      <td>regular</td>\n      <td>round neck</td>\n      <td>casual</td>\n      <td>solid</td>\n      <td>solid</td>\n      <td>short sleeves</td>\n      <td>NaN</td>\n      <td>knitted</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 12 columns</p>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"class LabelEncoderDict:\n    def __init__(self):\n        self.encoders = {}\n        \n    def fit(self, df, columns):\n        \"\"\"Fit label encoders for each column\"\"\"\n        for col in columns:\n            le = LabelEncoder()\n            # Include NaN as a unique label by appending it to valid labels\n            valid_labels = df[col].dropna().unique().tolist()\n            valid_labels.append('NaN')  # Assign a label for NaN\n            le.fit(valid_labels)\n            self.encoders[col] = le\n            \n    def transform(self, df, columns):\n        \"\"\"Transform labels using fitted encoders\"\"\"\n        encoded = np.zeros((len(df), len(columns)))\n        for i, col in enumerate(columns):\n            series = df[col].copy()\n            # Replace NaNs with the string 'NaN' so they can be encoded\n            series = series.fillna('NaN')\n            encoded[:, i] = self.encoders[col].transform(series)\n        return encoded\n    \n    def get_num_classes(self, column):\n        \"\"\"Get number of classes for a specific column\"\"\"\n        return len(self.encoders[column].classes_)\n\n\nclass MultiLabelImageDataset(Dataset):\n    def __init__(self, df, image_dir, transform=None, attr_columns=None):\n        \"\"\"\n        Args:\n            df: DataFrame containing image names and attributes\n            image_dir: Directory containing the images\n            transform: torchvision transforms\n            attr_columns: List of column names for attributes\n        \"\"\"\n        self.df = df\n        self.image_dir = image_dir\n        self.transform = transform\n        self.attr_columns = attr_columns if attr_columns else [f'attr_{i}' for i in range(1, 11)]\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        # Get image path\n        img_name = str(self.df.iloc[idx]['id']).zfill(6)\n        img_path = os.path.join(self.image_dir, f\"{img_name}.jpg\")\n        \n        # Load image\n        try:\n            image = Image.open(img_path).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image {img_path}: {e}\")\n            image = Image.new('RGB', (224, 224))  # Blank image on error\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        # Ensure labels are integers and convert to tensor\n        labels = torch.tensor(self.df.iloc[idx][self.attr_columns].astype(int).values, dtype=torch.long)\n        \n        return image, labels\n\n\ndef prepare_data(df, image_dir, batch_size=32, test_size=0.2):\n    \"\"\"\n    Prepare data loaders and label encoders\n    \"\"\"\n    # Define attribute columns\n    attr_columns = [f'attr_{i}' for i in range(1, 11)]\n    \n    # Create and fit label encoders\n    label_encoders = LabelEncoderDict()\n    label_encoders.fit(df, attr_columns)\n    \n    # Transform labels\n    encoded_labels = label_encoders.transform(df, attr_columns)\n    df_encoded = df.copy()\n    for i, col in enumerate(attr_columns):\n        df_encoded[col] = encoded_labels[:, i]\n    \n    # Split data\n    train_df, val_df = train_test_split(df_encoded, test_size=test_size, random_state=42)\n    \n    # Define transforms\n    transform = transforms.Compose([\n        transforms.Resize((256, 512)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                           std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create datasets\n    train_dataset = MultiLabelImageDataset(\n        train_df,\n        image_dir,\n        transform=transform,\n        attr_columns=attr_columns\n    )\n    \n    val_dataset = MultiLabelImageDataset(\n        val_df,\n        image_dir,\n        transform=transform,\n        attr_columns=attr_columns\n    )\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    \n    # Get number of classes for each attribute\n    num_classes_per_attr = [label_encoders.get_num_classes(col) for col in attr_columns]\n    \n    return train_loader, val_loader, label_encoders, num_classes_per_attr\n\nclass MultiLabelClassifier(nn.Module):\n    def __init__(self, num_classes_per_attr, pretrained=True):\n        super(MultiLabelClassifier, self).__init__()\n        self.backbone = models.resnet50(pretrained=pretrained)\n        num_features = self.backbone.fc.in_features\n        self.backbone = torch.nn.Sequential(*(list(self.backbone.children())[:-1]))\n        \n        # Create separate classifier heads for each attribute\n        self.classifier_heads = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(num_features, 512),\n                nn.ReLU(),\n                nn.Dropout(0.3),\n                nn.Linear(512, num_classes)\n            ) for num_classes in num_classes_per_attr\n        ])\n\n    def forward(self, x):\n        features = self.backbone(x)\n        features = features.view(features.size(0), -1)\n        return [head(features) for head in self.classifier_heads]\n\nclass MultiLabelCELoss(nn.Module):\n    def __init__(self):\n        super(MultiLabelCELoss, self).__init__()\n        self.criterion = nn.CrossEntropyLoss()\n\n    def forward(self, outputs, targets):\n        # outputs is a list of predictions for each label\n        # targets is a tensor of shape (batch_size, num_labels)\n        loss = 0\n        for i, output in enumerate(outputs):\n            loss += self.criterion(output, targets[:, i])\n        return loss / len(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T06:56:42.550869Z","iopub.execute_input":"2024-10-20T06:56:42.551443Z","iopub.status.idle":"2024-10-20T06:56:42.576126Z","shell.execute_reply.started":"2024-10-20T06:56:42.551398Z","shell.execute_reply":"2024-10-20T06:56:42.575255Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class MultiLabelClassifier(nn.Module):\n    def __init__(self, num_classes_per_attr, pretrained=True):\n        super(MultiLabelClassifier, self).__init__()\n        # Use a stronger backbone\n        self.backbone = models.efficientnet_b2(pretrained=pretrained)\n        num_features = self.backbone.classifier[1].in_features\n        self.backbone = torch.nn.Sequential(*(list(self.backbone.children())[:-1]))\n        \n        # Add batch normalization and feature extraction layers\n        self.feature_extractor = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Flatten(),\n            nn.BatchNorm1d(num_features),\n            nn.Dropout(0.5),\n            nn.Linear(num_features, 1024),\n            nn.ReLU(),\n            nn.BatchNorm1d(1024),\n            nn.Dropout(0.4)\n        )\n        \n        # Create separate classifier heads with shared features\n        self.shared_features = nn.Sequential(\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(0.3)\n        )\n        \n        # Classifier heads with attention mechanism\n        self.classifier_heads = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(512, 256),\n                nn.ReLU(),\n                nn.BatchNorm1d(256),\n                nn.Dropout(0.2),\n                nn.Linear(256, num_classes)\n            ) for num_classes in num_classes_per_attr\n        ])\n        \n        # Attention modules for each attribute\n        self.attention_modules = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(512, 1),\n                nn.Sigmoid()\n            ) for _ in num_classes_per_attr\n        ])\n\n    def forward(self, x):\n        # Extract features\n        features = self.backbone(x)\n        features = self.feature_extractor(features)\n        \n        # Get shared features\n        shared = self.shared_features(features)\n        \n        # Apply attention and get predictions\n        outputs = []\n        for attention_module, classifier_head in zip(self.attention_modules, self.classifier_heads):\n            # Apply attention\n            attention_weights = attention_module(shared)\n            attended_features = shared * attention_weights\n            \n            # Get predictions\n            output = classifier_head(attended_features)\n            outputs.append(output)\n            \n        return outputs\n\nclass WeightedMultiLabelLoss(nn.Module):\n    def __init__(self, num_classes_per_attr, device):\n        super(WeightedMultiLabelLoss, self).__init__()\n        self.num_attributes = len(num_classes_per_attr)\n        self.class_weights = [None] * self.num_attributes\n        self.device = device\n        \n    def update_weights(self, dataset):\n        \"\"\"Update class weights based on class distribution\"\"\"\n        for i in range(self.num_attributes):\n            labels = [item[1][i].item() for item in dataset]\n            class_counts = torch.bincount(torch.tensor(labels))\n            weights = 1.0 / class_counts.float()\n            weights = weights / weights.sum()\n            self.class_weights[i] = weights.to(self.device)\n    \n    def forward(self, outputs, targets):\n        loss = 0\n        batch_size = targets.size(0)\n        \n        for i, (output, weights) in enumerate(zip(outputs, self.class_weights)):\n            # Cross entropy with class weights\n            ce_loss = F.cross_entropy(output, targets[:, i], weight=weights)\n            \n            # Focal loss component\n            pt = torch.exp(-ce_loss)\n            focal_loss = (1 - pt) ** 2 * ce_loss\n            \n            # Add label smoothing\n            smooth_loss = -torch.mean(torch.log_softmax(output, dim=1).mean(dim=1))\n            \n            # Combine losses\n            combined_loss = 0.8 * focal_loss + 0.1 * ce_loss + 0.1 * smooth_loss\n            loss += combined_loss\n            \n        return loss / self.num_attributes\n\ndef train_model(model, train_loader, val_loader, num_epochs, num_classes_per_attr):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    \n    # Initialize loss and optimizer\n    criterion = WeightedMultiLabelLoss(num_classes_per_attr, device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer, \n        max_lr=1e-3,\n        epochs=num_epochs,\n        steps_per_epoch=len(train_loader)\n    )\n    \n    # Update class weights based on training data\n    criterion.update_weights(train_loader.dataset)\n    \n    # Early stopping\n    best_val_loss = float('inf')\n    patience = 5\n    patience_counter = 0\n    \n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        train_loss = 0\n        correct_predictions = [0] * len(num_classes_per_attr)\n        total_predictions = 0\n        \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            # Mixed precision training\n            with torch.cuda.amp.autocast():\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            scheduler.step()\n            \n            train_loss += loss.item()\n            \n            # Calculate accuracy for each attribute\n            for i, output in enumerate(outputs):\n                _, predicted = torch.max(output.data, 1)\n                correct_predictions[i] += (predicted == labels[:, i]).sum().item()\n            total_predictions += labels.size(0)\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0\n        val_correct_predictions = [0] * len(num_classes_per_attr)\n        val_total_predictions = 0\n        \n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                \n                for i, output in enumerate(outputs):\n                    _, predicted = torch.max(output.data, 1)\n                    val_correct_predictions[i] += (predicted == labels[:, i]).sum().item()\n                val_total_predictions += labels.size(0)\n        \n        # Print metrics\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n        \n        # Initialize total correct predictions\n        total_correct_predictions = 0\n        total_val_correct_predictions = 0\n        \n        for i in range(len(num_classes_per_attr)):\n            train_acc = 100 * correct_predictions[i] / total_predictions\n            val_acc = 100 * val_correct_predictions[i] / val_total_predictions\n            print(f'Attribute {i+1} - Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n        \n            # Sum correct predictions for overall accuracy\n            total_correct_predictions += correct_predictions[i]\n            total_val_correct_predictions += val_correct_predictions[i]\n        \n        # Calculate overall accuracy\n        overall_train_acc = 100 * total_correct_predictions / (total_predictions * len(num_classes_per_attr))\n        overall_val_acc = 100 * total_val_correct_predictions / (val_total_predictions * len(num_classes_per_attr))\n        \n        # Print overall accuracy\n        print(f'Overall Train Accuracy: {overall_train_acc:.2f}%')\n        print(f'Overall Validation Accuracy: {overall_val_acc:.2f}%')\n        \n        # Early stopping check\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            # Save best model\n            torch.save(model.state_dict(), 'best_model.pth')\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered\")\n                break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T06:56:42.577436Z","iopub.execute_input":"2024-10-20T06:56:42.577979Z","iopub.status.idle":"2024-10-20T06:56:42.610648Z","shell.execute_reply.started":"2024-10-20T06:56:42.577937Z","shell.execute_reply":"2024-10-20T06:56:42.609804Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def main(df):\n    \n    # Set image directory\n    image_dir = '/kaggle/input/visual-taxonomy/train_images'\n    \n    # Prepare data\n    train_loader, val_loader, label_encoders, num_classes_per_attr = prepare_data(\n        df, \n        image_dir,\n        batch_size=64\n    )\n    \n    # Initialize model\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    # Initialize the model\n    model = MultiLabelClassifier(num_classes_per_attr)\n    \n    # Define loss and optimizer\n    criterion = MultiLabelCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    # Train the model\n    train_model(model, train_loader, val_loader, num_epochs=50, num_classes_per_attr=num_classes_per_attr)\n\n    # Save label encoders for future use\n    import pickle\n    with open('label_encoders.pkl', 'wb') as f:\n        pickle.dump(label_encoders, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T06:56:42.611688Z","iopub.execute_input":"2024-10-20T06:56:42.611995Z","iopub.status.idle":"2024-10-20T06:56:42.624260Z","shell.execute_reply.started":"2024-10-20T06:56:42.611951Z","shell.execute_reply":"2024-10-20T06:56:42.623410Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"main(df_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-20T06:56:42.626523Z","iopub.execute_input":"2024-10-20T06:56:42.626803Z","iopub.status.idle":"2024-10-20T07:01:35.253161Z","shell.execute_reply.started":"2024-10-20T06:56:42.626773Z","shell.execute_reply":"2024-10-20T07:01:35.252185Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n100%|██████████| 35.2M/35.2M [00:01<00:00, 36.4MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\nTraining Loss: 1.4367\nValidation Loss: 1.2593\nAttribute 1 - Train Acc: 7.00%, Val Acc: 6.00%\nAttribute 2 - Train Acc: 23.25%, Val Acc: 9.00%\nAttribute 3 - Train Acc: 33.12%, Val Acc: 31.50%\nAttribute 4 - Train Acc: 11.50%, Val Acc: 3.50%\nAttribute 5 - Train Acc: 32.00%, Val Acc: 69.50%\nAttribute 6 - Train Acc: 25.75%, Val Acc: 25.50%\nAttribute 7 - Train Acc: 15.50%, Val Acc: 8.50%\nAttribute 8 - Train Acc: 22.50%, Val Acc: 42.50%\nAttribute 9 - Train Acc: 20.38%, Val Acc: 15.50%\nAttribute 10 - Train Acc: 13.62%, Val Acc: 5.50%\nOverall Train Accuracy: 20.46%\nOverall Validation Accuracy: 21.70%\nEpoch 2/50\nTraining Loss: 1.4054\nValidation Loss: 1.2598\nAttribute 1 - Train Acc: 7.00%, Val Acc: 10.00%\nAttribute 2 - Train Acc: 22.50%, Val Acc: 11.00%\nAttribute 3 - Train Acc: 35.50%, Val Acc: 33.50%\nAttribute 4 - Train Acc: 11.88%, Val Acc: 3.50%\nAttribute 5 - Train Acc: 37.25%, Val Acc: 54.50%\nAttribute 6 - Train Acc: 28.62%, Val Acc: 31.00%\nAttribute 7 - Train Acc: 16.62%, Val Acc: 10.00%\nAttribute 8 - Train Acc: 22.88%, Val Acc: 42.50%\nAttribute 9 - Train Acc: 21.38%, Val Acc: 16.00%\nAttribute 10 - Train Acc: 12.25%, Val Acc: 3.00%\nOverall Train Accuracy: 21.59%\nOverall Validation Accuracy: 21.50%\nEpoch 3/50\nTraining Loss: 1.3383\nValidation Loss: 1.2319\nAttribute 1 - Train Acc: 11.50%, Val Acc: 11.00%\nAttribute 2 - Train Acc: 25.88%, Val Acc: 19.00%\nAttribute 3 - Train Acc: 38.00%, Val Acc: 39.50%\nAttribute 4 - Train Acc: 14.00%, Val Acc: 4.00%\nAttribute 5 - Train Acc: 35.75%, Val Acc: 42.50%\nAttribute 6 - Train Acc: 33.38%, Val Acc: 38.00%\nAttribute 7 - Train Acc: 17.12%, Val Acc: 16.50%\nAttribute 8 - Train Acc: 22.50%, Val Acc: 42.50%\nAttribute 9 - Train Acc: 20.88%, Val Acc: 23.00%\nAttribute 10 - Train Acc: 14.62%, Val Acc: 5.50%\nOverall Train Accuracy: 23.36%\nOverall Validation Accuracy: 24.15%\nEpoch 4/50\nTraining Loss: 1.2699\nValidation Loss: 1.1708\nAttribute 1 - Train Acc: 10.25%, Val Acc: 11.50%\nAttribute 2 - Train Acc: 28.00%, Val Acc: 25.00%\nAttribute 3 - Train Acc: 42.75%, Val Acc: 47.00%\nAttribute 4 - Train Acc: 16.00%, Val Acc: 12.00%\nAttribute 5 - Train Acc: 42.12%, Val Acc: 45.00%\nAttribute 6 - Train Acc: 36.75%, Val Acc: 50.50%\nAttribute 7 - Train Acc: 18.50%, Val Acc: 22.50%\nAttribute 8 - Train Acc: 28.00%, Val Acc: 44.00%\nAttribute 9 - Train Acc: 26.88%, Val Acc: 29.50%\nAttribute 10 - Train Acc: 13.75%, Val Acc: 9.00%\nOverall Train Accuracy: 26.30%\nOverall Validation Accuracy: 29.60%\nEpoch 5/50\nTraining Loss: 1.1674\nValidation Loss: 1.0965\nAttribute 1 - Train Acc: 10.62%, Val Acc: 13.50%\nAttribute 2 - Train Acc: 31.88%, Val Acc: 33.50%\nAttribute 3 - Train Acc: 45.50%, Val Acc: 52.00%\nAttribute 4 - Train Acc: 20.38%, Val Acc: 23.00%\nAttribute 5 - Train Acc: 42.62%, Val Acc: 35.00%\nAttribute 6 - Train Acc: 43.00%, Val Acc: 57.50%\nAttribute 7 - Train Acc: 24.38%, Val Acc: 36.00%\nAttribute 8 - Train Acc: 27.88%, Val Acc: 46.00%\nAttribute 9 - Train Acc: 29.88%, Val Acc: 37.50%\nAttribute 10 - Train Acc: 15.38%, Val Acc: 9.50%\nOverall Train Accuracy: 29.15%\nOverall Validation Accuracy: 34.35%\nEpoch 6/50\nTraining Loss: 1.0578\nValidation Loss: 1.0504\nAttribute 1 - Train Acc: 16.00%, Val Acc: 16.50%\nAttribute 2 - Train Acc: 36.25%, Val Acc: 39.00%\nAttribute 3 - Train Acc: 49.25%, Val Acc: 58.50%\nAttribute 4 - Train Acc: 24.88%, Val Acc: 28.00%\nAttribute 5 - Train Acc: 45.12%, Val Acc: 42.00%\nAttribute 6 - Train Acc: 44.88%, Val Acc: 62.50%\nAttribute 7 - Train Acc: 30.75%, Val Acc: 45.50%\nAttribute 8 - Train Acc: 33.12%, Val Acc: 50.00%\nAttribute 9 - Train Acc: 36.12%, Val Acc: 44.00%\nAttribute 10 - Train Acc: 18.00%, Val Acc: 11.00%\nOverall Train Accuracy: 33.44%\nOverall Validation Accuracy: 39.70%\nEpoch 7/50\nTraining Loss: 0.9784\nValidation Loss: 0.9767\nAttribute 1 - Train Acc: 20.50%, Val Acc: 17.00%\nAttribute 2 - Train Acc: 42.38%, Val Acc: 39.00%\nAttribute 3 - Train Acc: 54.38%, Val Acc: 59.50%\nAttribute 4 - Train Acc: 29.12%, Val Acc: 32.00%\nAttribute 5 - Train Acc: 48.38%, Val Acc: 39.00%\nAttribute 6 - Train Acc: 48.50%, Val Acc: 58.50%\nAttribute 7 - Train Acc: 36.88%, Val Acc: 52.00%\nAttribute 8 - Train Acc: 37.00%, Val Acc: 51.50%\nAttribute 9 - Train Acc: 41.00%, Val Acc: 45.00%\nAttribute 10 - Train Acc: 20.88%, Val Acc: 17.00%\nOverall Train Accuracy: 37.90%\nOverall Validation Accuracy: 41.05%\nEpoch 8/50\nTraining Loss: 0.9014\nValidation Loss: 0.9607\nAttribute 1 - Train Acc: 21.38%, Val Acc: 23.00%\nAttribute 2 - Train Acc: 44.50%, Val Acc: 37.00%\nAttribute 3 - Train Acc: 58.00%, Val Acc: 59.00%\nAttribute 4 - Train Acc: 34.25%, Val Acc: 36.00%\nAttribute 5 - Train Acc: 48.75%, Val Acc: 37.00%\nAttribute 6 - Train Acc: 52.75%, Val Acc: 61.00%\nAttribute 7 - Train Acc: 41.12%, Val Acc: 56.00%\nAttribute 8 - Train Acc: 43.62%, Val Acc: 45.00%\nAttribute 9 - Train Acc: 45.50%, Val Acc: 45.00%\nAttribute 10 - Train Acc: 21.38%, Val Acc: 15.00%\nOverall Train Accuracy: 41.12%\nOverall Validation Accuracy: 41.40%\nEpoch 9/50\nTraining Loss: 0.7963\nValidation Loss: 0.9620\nAttribute 1 - Train Acc: 30.38%, Val Acc: 25.00%\nAttribute 2 - Train Acc: 48.62%, Val Acc: 46.00%\nAttribute 3 - Train Acc: 62.00%, Val Acc: 64.00%\nAttribute 4 - Train Acc: 35.62%, Val Acc: 39.50%\nAttribute 5 - Train Acc: 49.62%, Val Acc: 51.50%\nAttribute 6 - Train Acc: 56.88%, Val Acc: 66.50%\nAttribute 7 - Train Acc: 52.88%, Val Acc: 62.00%\nAttribute 8 - Train Acc: 48.38%, Val Acc: 48.00%\nAttribute 9 - Train Acc: 48.88%, Val Acc: 47.00%\nAttribute 10 - Train Acc: 22.12%, Val Acc: 18.00%\nOverall Train Accuracy: 45.54%\nOverall Validation Accuracy: 46.75%\nEpoch 10/50\nTraining Loss: 0.7179\nValidation Loss: 0.9501\nAttribute 1 - Train Acc: 33.50%, Val Acc: 33.00%\nAttribute 2 - Train Acc: 52.38%, Val Acc: 48.50%\nAttribute 3 - Train Acc: 65.50%, Val Acc: 63.00%\nAttribute 4 - Train Acc: 41.38%, Val Acc: 39.50%\nAttribute 5 - Train Acc: 54.25%, Val Acc: 58.00%\nAttribute 6 - Train Acc: 60.38%, Val Acc: 58.50%\nAttribute 7 - Train Acc: 55.62%, Val Acc: 58.50%\nAttribute 8 - Train Acc: 50.88%, Val Acc: 62.50%\nAttribute 9 - Train Acc: 51.25%, Val Acc: 52.00%\nAttribute 10 - Train Acc: 25.38%, Val Acc: 16.00%\nOverall Train Accuracy: 49.05%\nOverall Validation Accuracy: 48.95%\nEpoch 11/50\nTraining Loss: 0.6722\nValidation Loss: 0.9989\nAttribute 1 - Train Acc: 39.62%, Val Acc: 33.50%\nAttribute 2 - Train Acc: 55.62%, Val Acc: 50.50%\nAttribute 3 - Train Acc: 65.62%, Val Acc: 68.50%\nAttribute 4 - Train Acc: 46.00%, Val Acc: 43.50%\nAttribute 5 - Train Acc: 60.62%, Val Acc: 59.50%\nAttribute 6 - Train Acc: 61.38%, Val Acc: 62.50%\nAttribute 7 - Train Acc: 55.50%, Val Acc: 57.00%\nAttribute 8 - Train Acc: 53.88%, Val Acc: 57.00%\nAttribute 9 - Train Acc: 55.38%, Val Acc: 53.50%\nAttribute 10 - Train Acc: 27.00%, Val Acc: 17.50%\nOverall Train Accuracy: 52.06%\nOverall Validation Accuracy: 50.30%\nEpoch 12/50\nTraining Loss: 0.6219\nValidation Loss: 1.0070\nAttribute 1 - Train Acc: 44.50%, Val Acc: 38.50%\nAttribute 2 - Train Acc: 58.50%, Val Acc: 49.50%\nAttribute 3 - Train Acc: 66.62%, Val Acc: 61.00%\nAttribute 4 - Train Acc: 46.62%, Val Acc: 50.00%\nAttribute 5 - Train Acc: 59.00%, Val Acc: 56.00%\nAttribute 6 - Train Acc: 66.62%, Val Acc: 59.00%\nAttribute 7 - Train Acc: 61.00%, Val Acc: 57.50%\nAttribute 8 - Train Acc: 57.75%, Val Acc: 58.00%\nAttribute 9 - Train Acc: 57.25%, Val Acc: 53.50%\nAttribute 10 - Train Acc: 31.50%, Val Acc: 18.50%\nOverall Train Accuracy: 54.94%\nOverall Validation Accuracy: 50.15%\nEpoch 13/50\nTraining Loss: 0.6021\nValidation Loss: 1.0798\nAttribute 1 - Train Acc: 47.88%, Val Acc: 43.50%\nAttribute 2 - Train Acc: 60.62%, Val Acc: 48.50%\nAttribute 3 - Train Acc: 67.50%, Val Acc: 60.50%\nAttribute 4 - Train Acc: 52.75%, Val Acc: 43.50%\nAttribute 5 - Train Acc: 64.62%, Val Acc: 56.50%\nAttribute 6 - Train Acc: 68.62%, Val Acc: 62.50%\nAttribute 7 - Train Acc: 64.88%, Val Acc: 59.50%\nAttribute 8 - Train Acc: 61.88%, Val Acc: 53.00%\nAttribute 9 - Train Acc: 56.75%, Val Acc: 49.50%\nAttribute 10 - Train Acc: 33.62%, Val Acc: 25.50%\nOverall Train Accuracy: 57.91%\nOverall Validation Accuracy: 50.25%\nEpoch 14/50\nTraining Loss: 0.5716\nValidation Loss: 1.1244\nAttribute 1 - Train Acc: 57.00%, Val Acc: 45.50%\nAttribute 2 - Train Acc: 61.12%, Val Acc: 49.00%\nAttribute 3 - Train Acc: 67.88%, Val Acc: 63.00%\nAttribute 4 - Train Acc: 52.62%, Val Acc: 51.00%\nAttribute 5 - Train Acc: 61.62%, Val Acc: 60.50%\nAttribute 6 - Train Acc: 66.12%, Val Acc: 57.00%\nAttribute 7 - Train Acc: 60.50%, Val Acc: 61.00%\nAttribute 8 - Train Acc: 63.38%, Val Acc: 55.00%\nAttribute 9 - Train Acc: 61.12%, Val Acc: 58.00%\nAttribute 10 - Train Acc: 38.62%, Val Acc: 26.50%\nOverall Train Accuracy: 59.00%\nOverall Validation Accuracy: 52.65%\nEpoch 15/50\nTraining Loss: 0.5556\nValidation Loss: 1.0713\nAttribute 1 - Train Acc: 58.38%, Val Acc: 40.00%\nAttribute 2 - Train Acc: 59.75%, Val Acc: 50.50%\nAttribute 3 - Train Acc: 71.62%, Val Acc: 67.00%\nAttribute 4 - Train Acc: 52.00%, Val Acc: 52.00%\nAttribute 5 - Train Acc: 68.75%, Val Acc: 55.50%\nAttribute 6 - Train Acc: 70.25%, Val Acc: 63.50%\nAttribute 7 - Train Acc: 66.12%, Val Acc: 59.50%\nAttribute 8 - Train Acc: 63.12%, Val Acc: 59.00%\nAttribute 9 - Train Acc: 67.25%, Val Acc: 60.50%\nAttribute 10 - Train Acc: 38.38%, Val Acc: 25.00%\nOverall Train Accuracy: 61.56%\nOverall Validation Accuracy: 53.25%\nEarly stopping triggered\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}