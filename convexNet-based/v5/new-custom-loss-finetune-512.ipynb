{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-11-09T06:13:38.069640Z","iopub.status.busy":"2024-11-09T06:13:38.065942Z","iopub.status.idle":"2024-11-09T06:13:56.242944Z","shell.execute_reply":"2024-11-09T06:13:56.241987Z","shell.execute_reply.started":"2024-11-09T06:13:38.069571Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","import os\n","from PIL import Image\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","import torch.nn.functional as F\n","import random\n","import cv2\n","from transformers import AutoImageProcessor,ConvNextModel\n","from tqdm import tqdm \n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from itertools import cycle\n","import math\n","from tqdm import tqdm "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-11-09T06:13:56.245088Z","iopub.status.busy":"2024-11-09T06:13:56.244562Z","iopub.status.idle":"2024-11-09T06:13:56.251695Z","shell.execute_reply":"2024-11-09T06:13:56.250692Z","shell.execute_reply.started":"2024-11-09T06:13:56.245054Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# pre-defined\n","category_dict = {'c1':'Men Tshirts', 'c2':'Sarees', 'c3': 'Kurtis', 'c4': 'Women Tshirts', 'c5': 'Women Tops & Tunics'}\n","semi_classes_dict = {'c1':[4, 2, 2, 3, 2], 'c2':[4, 6, 3, 8, 4, 3, 4, 5, 9, 2], 'c3': [13, 2, 2, 2, 2, 2, 2, 3, 2], 'c4': [7, 3, 3, 3, 6, 3, 2, 2], 'c5': [12, 4, 2, 7, 2, 3, 6, 4, 4, 6]}"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-11-09T06:13:56.253821Z","iopub.status.busy":"2024-11-09T06:13:56.253274Z","iopub.status.idle":"2024-11-09T06:13:56.277430Z","shell.execute_reply":"2024-11-09T06:13:56.276593Z","shell.execute_reply.started":"2024-11-09T06:13:56.253770Z"},"trusted":true},"outputs":[],"source":["# set paths as per the set-up and Hyperparameters\n","input_path = \"/kaggle/input/meesho\"\n","working_path = \"/kaggle/working\"\n","test_c_name = \"c4\"\n","NUM_EPOCH = 4\n","NUM_ATTR_EPOCHS = 1\n","LEARNING_RATE = 0.001\n","NUM_OF_SEMI_CLASSES_OF_COLUMNS = semi_classes_dict[test_c_name]"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-11-09T06:13:56.280184Z","iopub.status.busy":"2024-11-09T06:13:56.279865Z","iopub.status.idle":"2024-11-09T06:13:56.492714Z","shell.execute_reply":"2024-11-09T06:13:56.491652Z","shell.execute_reply.started":"2024-11-09T06:13:56.280142Z"},"trusted":true},"outputs":[],"source":["df_train = pd.read_csv(f'{input_path}/train.csv')\n","df_test = pd.read_csv(f'{input_path}/test.csv')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-11-09T06:13:56.494416Z","iopub.status.busy":"2024-11-09T06:13:56.494035Z","iopub.status.idle":"2024-11-09T06:13:56.512709Z","shell.execute_reply":"2024-11-09T06:13:56.511653Z","shell.execute_reply.started":"2024-11-09T06:13:56.494367Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array(['Men Tshirts', 'Sarees', 'Kurtis', 'Women Tshirts',\n","       'Women Tops & Tunics'], dtype=object)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df_train['Category'].unique()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-11-09T06:13:56.514374Z","iopub.status.busy":"2024-11-09T06:13:56.514003Z","iopub.status.idle":"2024-11-09T06:13:56.522143Z","shell.execute_reply":"2024-11-09T06:13:56.521132Z","shell.execute_reply.started":"2024-11-09T06:13:56.514312Z"},"trusted":true},"outputs":[],"source":["def do_preprocessing(test_c_name, test_category):\n","    df_temp = df_train[df_train['Category'] == test_category]\n","    df_sub = df_train[df_train['Category'] == test_category]\n","    df_sub.dropna(axis=1, how='all', inplace=True)\n","    temp = []\n","    for i in range(1,len(df_sub.columns)-2):\n","        temp.append(len(df_sub[f'attr_{i}'].unique().tolist())-1)\n","    print(f\"Number of features for original df: {temp}\")\n","    print()\n","    df_sub.dropna(axis=0, how='any', inplace=True)\n","    temp = []\n","    for i in range(1,len(df_sub.columns)-2):\n","        temp.append(len(df_sub[f'attr_{i}'].unique().tolist()))\n","    print(f\"Number of features for new df: {temp}\")\n","    return df_sub, df_temp"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-11-09T06:13:56.524142Z","iopub.status.busy":"2024-11-09T06:13:56.523771Z","iopub.status.idle":"2024-11-09T06:13:56.547821Z","shell.execute_reply":"2024-11-09T06:13:56.546945Z","shell.execute_reply.started":"2024-11-09T06:13:56.524101Z"},"trusted":true},"outputs":[],"source":["class LabelEncoderDict:\n","    def __init__(self):\n","        self.encoders = {}\n","        \n","    def fit(self, df, columns):\n","        \"\"\"Fit label encoders for each column\"\"\"\n","        for col in columns:\n","            le = LabelEncoder()\n","            valid_labels = df[col].unique().tolist()\n","            valid_labels = [x for x in valid_labels if not (isinstance(x, float) and math.isnan(x))]\n","            le.fit(valid_labels)\n","            self.encoders[col] = le\n","            \n","    def transform(self, df, columns):\n","        \"\"\"Transform labels using fitted encoders\"\"\"\n","        encoded = np.zeros((len(df), len(columns)))\n","        for i, col in enumerate(columns):\n","            series = df[col].copy()\n","            encoded[:, i] = self.encoders[col].transform(series)\n","        return encoded\n","    \n","    def get_num_classes(self, column):\n","        \"\"\"Get number of classes for a specific column\"\"\"\n","        return len(self.encoders[column].classes_)\n","\n","\n","class MultiLabelImageDataset(Dataset):\n","    def __init__(self, df, image_dir, transform_basic=None, transform_augmented=None, attr_columns=10, do_transform=True):\n","        self.df = df\n","        self.image_dir = image_dir\n","        self.transform_basic = transform_basic  # Basic transform without augmentation\n","        self.transform_augmented = transform_augmented  # Augmented transform with augmentation\n","        self.attr_columns = attr_columns\n","        self.do_transform = do_transform\n","        \n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    \n","    def __getitem__(self, idx):\n","        # Get image path\n","        img_name = str(self.df.iloc[idx]['id']).zfill(6)\n","        img_path = os.path.join(self.image_dir, f\"{img_name}.jpg\")\n","\n","        # Load image\n","        try:\n","            image = Image.open(img_path).convert('RGB')\n","        except Exception as e:\n","            print(f\"Error loading image {img_path}: {e}\")\n","            image = Image.new('RGB', (512, 512))\n","\n","\n","        if  self.do_transform and (random.random() > 0.5):\n","            if self.transform_augmented:\n","                image = self.transform_augmented(image)         \n","        else:\n","            if self.transform_basic:\n","                image = self.transform_basic(image)\n","        # Ensure labels are integers and convert to tensor\n","        labels = torch.tensor(self.df.iloc[idx][self.attr_columns].astype(int).values, dtype=torch.long)\n","\n","        return image, labels\n","\n","\n","\n","def prepare_data(df,label_encoders, image_dir, batch_size=32, test_size=0.1, num_attr_columns=10):\n","    \"\"\"\n","    Prepare data loaders and label encoders\n","    \"\"\"\n","      # TODO: Adjust number of columns\n","    \n","    # Transform labels\n","    attr_columns = [f'attr_{i}' for i in range(1, num_attr_columns+1)]\n","    encoded_labels = label_encoders.transform(df, attr_columns)\n","    df_encoded = df.copy()\n","    for i, col in enumerate(attr_columns):\n","        df_encoded[col] = encoded_labels[:, i]\n","    \n","    # Split data\n","    train_df, val_df = train_test_split(df_encoded, test_size=test_size, random_state=24)\n","    \n","    # Define transforms\n","    transform = transforms.Compose([\n","        transforms.Resize((512, 512)),  # Resize to 512x512\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])\n","    ])\n","    \n","    transform_augmented = transforms.Compose([\n","        transforms.Resize((512, 512)),  # Resize to 512x512\n","        transforms.RandomHorizontalFlip(p=0.9),  # 90% chance of horizontal flipping\n","        transforms.RandomRotation(degrees=5),  # Rotate by up to 20 degrees\n","        transforms.RandomResizedCrop(size=(512, 512), scale=(0.8, 1.0)),  # Randomly crop and resize\n","        transforms.RandomPerspective(distortion_scale=0.1, p=0.5),  # Apply perspective distortion\n","        transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.85, 1.15), shear=5),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # Create datasets\n","    train_dataset = MultiLabelImageDataset(\n","        train_df,\n","        image_dir,\n","        transform_basic=transform,\n","        transform_augmented=transform_augmented,\n","        attr_columns=attr_columns\n","    )\n","    \n","    val_dataset = MultiLabelImageDataset(\n","        val_df,\n","        image_dir,\n","        transform_basic=transform,\n","        transform_augmented=transform_augmented,\n","        attr_columns=attr_columns,\n","        do_transform=False\n","    )\n","    \n","    # Create dataloaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, drop_last=True)\n","    \n","    # Get number of classes for each attribute\n","    num_classes_per_attr = [label_encoders.get_num_classes(col) for col in attr_columns]\n","    \n","    return train_loader, val_loader, label_encoders, num_classes_per_attr"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-11-09T06:13:56.549362Z","iopub.status.busy":"2024-11-09T06:13:56.548969Z","iopub.status.idle":"2024-11-09T06:13:56.572610Z","shell.execute_reply":"2024-11-09T06:13:56.571722Z","shell.execute_reply.started":"2024-11-09T06:13:56.549267Z"},"trusted":true},"outputs":[],"source":["class MultiLabelClassifier(nn.Module):\n","    def __init__(self, num_classes_per_attr):\n","        super(MultiLabelClassifier, self).__init__()\n","        \n","        # Use ConvNeXt-Base with unfrozen backbone\n","        self.backbone = ConvNextModel.from_pretrained(\"facebook/convnext-base-384-22k-1k\")\n","        backbone_features = self.backbone.config.hidden_sizes[-1]  # 1024 for base model\n","        \n","        # Modified feature processing without fixed dimensions\n","        self.feature_processor = nn.Sequential(\n","            nn.Conv2d(backbone_features, 1024, kernel_size=1),\n","            nn.GELU(),\n","            nn.Dropout(0.1)\n","        )\n","        \n","        # Keep original ModuleList structure\n","        self.classifier_heads = nn.ModuleList()\n","        for num_classes in num_classes_per_attr:\n","            classifier_head = nn.Sequential(\n","                # First branch - Spatial attention\n","                nn.Sequential(\n","                    nn.Conv2d(1024, 512, kernel_size=3, padding=1, groups=32),\n","                    nn.GELU(),\n","                    nn.Conv2d(512, 512, kernel_size=3, padding=1, groups=32),\n","                    nn.GELU(),\n","                ),\n","                \n","                # Second branch - Channel attention (SE-like module)\n","                nn.Sequential(\n","                    nn.AdaptiveAvgPool2d(1),\n","                    nn.Flatten(),\n","                    nn.Linear(512, 128),\n","                    nn.GELU(),\n","                    nn.Linear(128, 512),\n","                    nn.Sigmoid(),\n","                ),\n","                \n","                # Combine branches and final classification\n","                nn.Sequential(\n","                    nn.AdaptiveAvgPool2d(1),\n","                    nn.Flatten(),\n","                    nn.Linear(512, 1024),\n","                    nn.LayerNorm(1024),\n","                    nn.GELU(),\n","                    nn.Dropout(0.2),\n","                    nn.Linear(1024, 512),\n","                    nn.LayerNorm(512),\n","                    nn.Sigmoid(),\n","                    nn.Dropout(0.1),\n","                    nn.Linear(512, num_classes)\n","                )\n","            )\n","            self.classifier_heads.append(classifier_head)\n","\n","    def freeze_backbone(self):\n","        \"\"\"Freeze the backbone model\"\"\"\n","        for param in self.backbone.parameters():\n","            param.requires_grad = False\n","            \n","    def unfreeze_backbone(self):\n","        \"\"\"Unfreeze the backbone model\"\"\"\n","        for param in self.backbone.parameters():\n","            param.requires_grad = True\n","            \n","    def freeze_feature_processor(self):\n","        \"\"\"Freeze the feature processor\"\"\"\n","        for param in self.feature_processor.parameters():\n","            param.requires_grad = False\n","            \n","    def unfreeze_feature_processor(self):\n","        \"\"\"Unfreeze the feature processor\"\"\"\n","        for param in self.feature_processor.parameters():\n","            param.requires_grad = True\n","\n","    def set_classifier_head_trainable(self, attr_index):\n","        \"\"\"\n","        Freeze all classifier heads except the specified one\n","        Args:\n","            attr_index: index of the attribute head to train (0 for attr_1, 1 for attr_2, etc.)\n","        \"\"\"\n","        for i, head in enumerate(self.classifier_heads):\n","            for param in head.parameters():\n","                param.requires_grad = (i == attr_index)\n","\n","    def freeze_all_except_head(self, attr_index):\n","        \"\"\"\n","        Freeze everything except the specified classifier head\n","        Args:\n","            attr_index: index of the attribute head to train (0 for attr_1, 1 for attr_2, etc.)\n","        \"\"\"\n","        self.freeze_backbone()\n","        self.freeze_feature_processor()\n","        self.set_classifier_head_trainable(attr_index)\n","        \n","    def unfreeze_all(self):\n","        \"\"\"Unfreeze all model components\"\"\"\n","        self.unfreeze_backbone()\n","        self.unfreeze_feature_processor()\n","        for head in self.classifier_heads:\n","            for param in head.parameters():\n","                param.requires_grad = True\n","\n","    def forward(self, x, attr_index=None, return_features=False):\n","        \"\"\"\n","        Forward pass with optional attribute-specific output\n","        Args:\n","            x: input tensor\n","            attr_index: specific attribute index to get output for (0 for attr_1, etc.)\n","            return_features: whether to return processed features\n","        \"\"\"\n","        # Extract features from ConvNeXt backbone\n","        features = self.backbone(x).last_hidden_state\n","        \n","        # Process features\n","        processed_features = self.feature_processor(features)\n","        \n","        if attr_index is not None:\n","            # Get output for specific attribute\n","            classifier_head = self.classifier_heads[attr_index]\n","            # Spatial attention branch\n","            spatial_features = classifier_head[0](processed_features)\n","            \n","            # Channel attention branch\n","            channel_attention = classifier_head[1](spatial_features)\n","            channel_attention = channel_attention.view(-1, 512, 1, 1)\n","            \n","            # Apply channel attention and get final output\n","            attended_features = spatial_features * channel_attention\n","            output = classifier_head[2](attended_features)\n","            \n","            if return_features:\n","                return output, processed_features\n","            return output\n","        \n","        # Get outputs for all attributes\n","        outputs = []\n","        for classifier_head in self.classifier_heads:\n","            # Spatial attention branch\n","            spatial_features = classifier_head[0](processed_features)\n","            \n","            # Channel attention branch\n","            channel_attention = classifier_head[1](spatial_features)\n","            channel_attention = channel_attention.view(-1, 512, 1, 1)\n","            \n","            # Apply channel attention and get final output\n","            attended_features = spatial_features * channel_attention\n","            outputs.append(classifier_head[2](attended_features))\n","            \n","        if return_features:\n","            return outputs, processed_features\n","        return outputs\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-11-09T06:13:56.574386Z","iopub.status.busy":"2024-11-09T06:13:56.574040Z","iopub.status.idle":"2024-11-09T06:13:56.622210Z","shell.execute_reply":"2024-11-09T06:13:56.621496Z","shell.execute_reply.started":"2024-11-09T06:13:56.574347Z"},"trusted":true},"outputs":[],"source":["class MultiLabelCELoss(nn.Module):\n","    def __init__(self):\n","        super(MultiLabelCELoss, self).__init__()\n","        self.criterion = nn.CrossEntropyLoss()\n","\n","    def forward(self, outputs, targets):\n","        loss = 0\n","        for i, output in enumerate(outputs):\n","            loss += self.criterion(output, targets[:, i])\n","        return loss / len(outputs)\n","    \n","class CombinedLoss(nn.Module):\n","    def __init__(self, lambda_mmd=0.1, chunk_size=1024):\n","        super(CombinedLoss, self).__init__()\n","        self.lambda_mmd = lambda_mmd\n","        self.chunk_size = chunk_size\n","        self.cross_entropy_loss = nn.CrossEntropyLoss()\n","\n","    def forward(self, outputs, labels, source_features, target_features):\n","        # Calculate Cross-Entropy Loss\n","        if isinstance(outputs, list):\n","            ce_loss = 0\n","            for i, output in enumerate(outputs):\n","                if isinstance(labels, list):\n","                    label = labels[i]\n","                else:\n","                    label = labels[:, i] if labels.dim() > 1 else labels\n","                ce_loss += self.cross_entropy_loss(output, label)\n","            ce_loss = ce_loss / len(outputs)\n","        else:\n","            ce_loss = self.cross_entropy_loss(outputs, labels)\n","\n","        # Calculate MMD Loss\n","        mmd_loss = self.maximum_mean_discrepancy(source_features, target_features)\n","        \n","        # Combine losses (without CORAL)\n","        total_loss = ce_loss + self.lambda_mmd * mmd_loss\n","        \n","        # Return zero for coral_loss to maintain compatibility\n","        return total_loss, ce_loss, mmd_loss, torch.tensor(0.0, device=ce_loss.device)\n","\n","    def gaussian_kernel(self, x, y, bandwidth):\n","        x = x.view(x.size(0), -1)\n","        y = y.view(y.size(0), -1)\n","        \n","        x_size = x.size(0)\n","        y_size = y.size(0)\n","        dim = x.size(1)\n","        \n","        x = x.unsqueeze(1)  # (x_size, 1, dim)\n","        y = y.unsqueeze(0)  # (1, y_size, dim)\n","        \n","        kernel_input = (x - y).pow(2).sum(2).div(2 * bandwidth * bandwidth)\n","        return torch.exp(-kernel_input)  # (x_size, y_size)\n","\n","    def maximum_mean_discrepancy(self, source_features, target_features):\n","        # Ensure inputs are 2D tensors\n","        if source_features.dim() > 2:\n","            source_features = source_features.view(source_features.size(0), -1)\n","        if target_features.dim() > 2:\n","            target_features = target_features.view(target_features.size(0), -1)\n","\n","        # Get sizes\n","        batch_source = source_features.size(0)\n","        batch_target = target_features.size(0)\n","        dim = source_features.size(1)\n","\n","        # Initialize MMD\n","        mmd = torch.tensor(0., device=source_features.device)\n","\n","        # Use multiple kernel bandwidths\n","        bandwidths = [dim * (2 ** i) for i in range(-3, 3)]\n","\n","        for bandwidth in bandwidths:\n","            # Process source-source\n","            source_sum = 0\n","            for i in range(0, batch_source, self.chunk_size):\n","                end = min(i + self.chunk_size, batch_source)\n","                chunk = source_features[i:end]\n","                kernel = self.gaussian_kernel(chunk, source_features, bandwidth)\n","                source_sum += kernel.sum().item()\n","\n","            # Process target-target\n","            target_sum = 0\n","            for i in range(0, batch_target, self.chunk_size):\n","                end = min(i + self.chunk_size, batch_target)\n","                chunk = target_features[i:end]\n","                kernel = self.gaussian_kernel(chunk, target_features, bandwidth)\n","                target_sum += kernel.sum().item()\n","\n","            # Process source-target\n","            cross_sum = 0\n","            for i in range(0, batch_source, self.chunk_size):\n","                s_end = min(i + self.chunk_size, batch_source)\n","                s_chunk = source_features[i:s_end]\n","                \n","                for j in range(0, batch_target, self.chunk_size):\n","                    t_end = min(j + self.chunk_size, batch_target)\n","                    t_chunk = target_features[j:t_end]\n","                    \n","                    kernel = self.gaussian_kernel(s_chunk, t_chunk, bandwidth)\n","                    cross_sum += kernel.sum().item()\n","\n","            # Calculate bandwidth contribution to MMD\n","            source_term = source_sum / (batch_source * batch_source)\n","            target_term = target_sum / (batch_target * batch_target)\n","            cross_term = 2 * cross_sum / (batch_source * batch_target)\n","            \n","            mmd = mmd + torch.tensor(source_term + target_term - cross_term, \n","                                   device=source_features.device)\n","\n","            # Clear cache\n","            if hasattr(torch.cuda, 'empty_cache'):\n","                torch.cuda.empty_cache()\n","\n","        return mmd / len(bandwidths)\n","\n","\n","\n","def train_model(model, train_loader, val_loader, num_epochs, num_classes_per_attr, model_type):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","    model = torch.nn.DataParallel(model)\n","    criterion = CombinedLoss()\n","    criterion2 = MultiLabelCELoss()\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","\n","    # Reduce learning rate on plateau\n","    lr_scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n","\n","    # Early stopping params\n","    early_stopping_patience = 4\n","    early_stopping_counter = 0\n","    best_val_overall_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0\n","        correct_predictions = [0] * len(num_classes_per_attr)\n","        total_predictions = 0\n","        overall_correct = 0\n","\n","        # Store true and predicted labels for metrics calculation\n","        train_true_labels = [[] for _ in range(len(num_classes_per_attr))]\n","        train_predicted_labels = [[] for _ in range(len(num_classes_per_attr))]\n","\n","        # Create cyclic iterator for validation data during training\n","        val_cycle = cycle(val_loader)\n","\n","        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\", unit=\"batch\") as t:\n","            for images, labels in t:\n","                # Get target domain batch\n","                target_images, _ = next(val_cycle)\n","                \n","                # Move data to device\n","                images = images.to(device)\n","                labels = labels.to(device)\n","                target_images = target_images.to(device)\n","\n","                # Forward pass on source domain (training data)\n","                outputs, source_features = model(images, return_features=True)\n","                \n","                # Forward pass on target domain (validation data)\n","                _, target_features = model(target_images, return_features=True)\n","\n","                # Calculate combined loss\n","                loss, ce_loss, mmd_loss, coral_loss = criterion(outputs, labels, source_features, target_features)\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                optimizer.step()\n","\n","                train_loss += loss.item()\n","                t.set_postfix(loss=loss.item())\n","\n","                all_labels_match = torch.ones(labels.size(0), dtype=torch.bool, device=device)\n","                for i, output in enumerate(outputs):\n","                    _, predicted = torch.max(output, 1)\n","                    correct_predictions[i] += (predicted == labels[:, i]).sum().item()\n","                    all_labels_match &= (predicted == labels[:, i])\n","\n","                    train_true_labels[i].extend(labels[:, i].cpu().numpy())\n","                    train_predicted_labels[i].extend(predicted.cpu().numpy())\n","\n","                overall_correct += all_labels_match.sum().item()\n","                total_predictions += labels.size(0)\n","\n","        # Validation phase\n","        model.eval()\n","        val_loss = 0\n","        val_correct_predictions = [0] * len(num_classes_per_attr)\n","        val_total_predictions = 0\n","        val_overall_correct = 0\n","\n","        val_true_labels = [[] for _ in range(len(num_classes_per_attr))]\n","        val_predicted_labels = [[] for _ in range(len(num_classes_per_attr))]\n","\n","        with tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\", unit=\"batch\") as v:\n","            with torch.no_grad():\n","                for images, labels in v:\n","                    images, labels = images.to(device), labels.to(device)\n","                    outputs = model(images,return_features=False)\n","                    loss = criterion2(outputs, labels)\n","                    val_loss += loss.item()\n","                    v.set_postfix(loss=loss.item())\n","\n","                    all_labels_match_val = torch.ones(labels.size(0), dtype=torch.bool, device=device)\n","                    for i, output in enumerate(outputs):\n","                        _, predicted = torch.max(output, 1)\n","                        val_correct_predictions[i] += (predicted == labels[:, i]).sum().item()\n","                        all_labels_match_val &= (predicted == labels[:, i])\n","\n","                        # Store validation labels for precision, recall, f1-score calculations\n","                        val_true_labels[i].extend(labels[:, i].cpu().numpy())\n","                        val_predicted_labels[i].extend(predicted.cpu().numpy())\n","\n","                    val_overall_correct += all_labels_match_val.sum().item()\n","                    val_total_predictions += labels.size(0)\n","                    \n","        print(f'Epoch {epoch+1}/{num_epochs}')\n","        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n","        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n","        \n","        for i in range(len(num_classes_per_attr)):\n","            train_acc = 100 * correct_predictions[i] / total_predictions\n","            val_acc = 100 * val_correct_predictions[i] / val_total_predictions\n","\n","            train_precision = precision_score(train_true_labels[i], train_predicted_labels[i], average='weighted')\n","            train_recall = recall_score(train_true_labels[i], train_predicted_labels[i], average='weighted')\n","            train_f1 = f1_score(train_true_labels[i], train_predicted_labels[i], average='weighted')\n","\n","            val_precision = precision_score(val_true_labels[i], val_predicted_labels[i], average='weighted')\n","            val_recall = recall_score(val_true_labels[i], val_predicted_labels[i], average='weighted')\n","            val_f1 = f1_score(val_true_labels[i], val_predicted_labels[i], average='weighted')\n","\n","            print(f'Attribute {i+1} - Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n","            print(f'Attribute {i+1} - Train Precision: {train_precision:.2f}, Train Recall: {train_recall:.2f}, Train F1-Score: {train_f1:.2f}')\n","            print(f'Attribute {i+1} - Val Precision: {val_precision:.2f}, Val Recall: {val_recall:.2f}, Val F1-Score: {val_f1:.2f}')\n","            print()\n","\n","        overall_train_acc = 100 * overall_correct / total_predictions\n","        overall_val_acc = 100 * val_overall_correct / val_total_predictions\n","        print(f'Overall Train Accuracy: {overall_train_acc:.2f}%')\n","        print(f'Overall Validation Accuracy: {overall_val_acc:.2f}%')\n","\n","        # Early stopping logic\n","        if overall_val_acc >= best_val_overall_acc:\n","            best_val_overall_acc = overall_val_acc\n","            torch.save(model.module.state_dict(), f'best_model_{model_type}.pth')\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","        \n","        lr_scheduler.step(overall_val_acc)\n","        \n","        if early_stopping_counter >= early_stopping_patience:\n","            print(\"Early stopping triggered\")\n","    torch.save(model.module.state_dict(), f'best_model_end_{model_type}.pth')\n","    return model\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-11-09T06:13:56.625407Z","iopub.status.busy":"2024-11-09T06:13:56.625095Z","iopub.status.idle":"2024-11-09T06:13:56.646058Z","shell.execute_reply":"2024-11-09T06:13:56.644959Z","shell.execute_reply.started":"2024-11-09T06:13:56.625374Z"},"trusted":true},"outputs":[],"source":["class SingleAttributeDataset(Dataset):\n","    def __init__(self, df, image_dir, attribute, transform_basic=None, transform_augmented=None, do_transform=True):\n","        self.df = df[df[attribute].notna()].reset_index(drop=True)\n","        self.image_dir = image_dir\n","        self.transform_basic = transform_basic\n","        self.transform_augmented = transform_augmented\n","        self.attribute = attribute\n","        self.do_transform = do_transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        # Get image path\n","        img_name = str(self.df.iloc[idx]['id']).zfill(6)\n","        img_path = os.path.join(self.image_dir, f\"{img_name}.jpg\")\n","\n","        # Load image\n","        try:\n","            image = Image.open(img_path).convert('RGB')\n","        except Exception as e:\n","            print(f\"Error loading image {img_path}: {e}\")\n","            image = Image.new('RGB', (512, 512))\n","\n","        # Apply transforms\n","        if self.do_transform and (random.random() > 0.5):\n","            if self.transform_augmented:\n","                image = self.transform_augmented(image)\n","        else:\n","            if self.transform_basic:\n","                image = self.transform_basic(image)\n","\n","        # Get label for this attribute\n","        label = torch.tensor(self.df.iloc[idx][self.attribute], dtype=torch.long)\n","\n","        return image, label\n","\n","def prepare_attribute_data(df, image_dir,label_encoders, batch_size=32, num_attr_columns=10, test_size=0.1):\n","    \"\"\"\n","    Prepare separate data loaders for each attribute\n","    \"\"\"\n","    # Define attribute columns\n","    attr_columns = [f'attr_{i}' for i in range(1, num_attr_columns+1)]\n","    \n","    # Transform labels for each attribute\n","    df_encoded = df.copy()\n","    for col in attr_columns:\n","        # Only encode non-null values\n","        mask = df_encoded[col].notna()\n","        df_encoded.loc[mask, col] = label_encoders.encoders[col].transform(df_encoded.loc[mask, col])\n","    \n","    # Define transforms\n","    transform = transforms.Compose([\n","        transforms.Resize((512, 512)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                           std=[0.229, 0.224, 0.225])\n","    ])\n","    \n","    transform_augmented = transforms.Compose([\n","        transforms.Resize((512, 512)),\n","        transforms.RandomHorizontalFlip(p=0.9),\n","        transforms.RandomRotation(degrees=5),\n","        transforms.RandomResizedCrop(size=(512, 512), scale=(0.8, 1.0)),\n","        transforms.RandomPerspective(distortion_scale=0.1, p=0.5),\n","        transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.85, 1.15), shear=5),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    # Create separate dataloaders for each attribute\n","    attribute_loaders = {}\n","    num_classes_per_attr = {}\n","    \n","    for attr in attr_columns:\n","        # Get data where this attribute is not null\n","        attr_df = df_encoded[df_encoded[attr].notna()].copy()\n","        \n","        # Split data for this attribute\n","        train_df, val_df = train_test_split(\n","            attr_df, \n","            test_size=test_size, \n","            stratify=attr_df[attr],  # Stratify by this attribute\n","            random_state=24\n","        )\n","        \n","        # Create datasets\n","        train_dataset = SingleAttributeDataset(\n","            train_df,\n","            image_dir,\n","            attr,\n","            transform_basic=transform,\n","            transform_augmented=transform_augmented,\n","            do_transform=True\n","        )\n","        \n","        val_dataset = SingleAttributeDataset(\n","            val_df,\n","            image_dir,\n","            attr,\n","            transform_basic=transform,\n","            transform_augmented=None, \n","            do_transform=False\n","        )\n","        \n","        # Create dataloaders\n","        train_loader = DataLoader(\n","            train_dataset, \n","            batch_size=batch_size, \n","            shuffle=True, \n","            drop_last=True\n","        )\n","        \n","        val_loader = DataLoader(\n","            val_dataset, \n","            batch_size=batch_size, \n","            shuffle=False,\n","            drop_last=True\n","        )\n","        \n","        # Store loaders for this attribute\n","        attribute_loaders[attr] = {\n","            'train': train_loader,\n","            'val': val_loader,\n","            'num_samples': len(attr_df)\n","        }\n","        \n","        # Store number of classes for this attribute\n","        num_classes_per_attr[attr] = label_encoders.get_num_classes(attr)\n","        \n","        print(f\"{attr}: {len(train_df)} train samples, {len(val_df)} val samples\")\n","    \n","    return attribute_loaders"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-11-09T06:13:56.647640Z","iopub.status.busy":"2024-11-09T06:13:56.647298Z","iopub.status.idle":"2024-11-09T06:13:56.665369Z","shell.execute_reply":"2024-11-09T06:13:56.664372Z","shell.execute_reply.started":"2024-11-09T06:13:56.647609Z"},"trusted":true},"outputs":[],"source":["def train_attribute_model(model, dataloaders, num_epochs, model_type, device='cuda'):\n","    \"\"\"\n","    Train a model for multi-attribute classification, handling layer freezing/unfreezing per attribute.\n","    \n","    Parameters:\n","    - model: The model wrapped in torch.nn.DataParallel (must have a method to freeze/unfreeze layers).\n","    - dataloaders: Dictionary containing 'train' and 'val' DataLoaders for each attribute.\n","    - num_epochs: Number of epochs to train the model.\n","    - device: The device to run the model on (default is 'cuda').\n","    \"\"\"\n","    for attr_index, attr in enumerate(dataloaders.keys()):\n","        train_loader = dataloaders[attr]['train']\n","        val_loader = dataloaders[attr]['val']\n","        \n","        best_val_acc = 0.0\n","        \n","        # Ensure model is detached from DataParallel before modifying layers\n","        model.module.freeze_all_except_head(attr_index)\n","        model.to(device)\n","\n","        optimizer = torch.optim.Adam(model.parameters())\n","        criterion = torch.nn.CrossEntropyLoss()  # Use the appropriate loss function\n","        \n","        for epoch in range(num_epochs):\n","            # Training phase\n","            model.train()\n","            train_loss = 0\n","            all_train_preds = []\n","            all_train_labels = []\n","\n","            for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} train {attr}\"):\n","                images, labels = images.to(device), labels.to(device)\n","                \n","                # Forward pass for the specific attribute\n","                outputs = model(images, attr_index=attr_index)\n","                loss = criterion(outputs, labels)\n","                train_loss += loss.item()\n","\n","                # Backward pass and optimization\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","                # Store predictions and labels for metrics\n","                _, preds = torch.max(outputs, 1)\n","                all_train_preds.extend(preds.cpu().numpy())\n","                all_train_labels.extend(labels.cpu().numpy())\n","\n","            # Calculate training metrics\n","            train_accuracy = accuracy_score(all_train_labels, all_train_preds)\n","            train_precision = precision_score(all_train_labels, all_train_preds, average='weighted')\n","            train_recall = recall_score(all_train_labels, all_train_preds, average='weighted')\n","            train_f1 = f1_score(all_train_labels, all_train_preds, average='weighted')\n","\n","            # Validation phase\n","            model.eval()\n","            val_loss = 0\n","            all_val_preds = []\n","            all_val_labels = []\n","\n","            with torch.no_grad():\n","                for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} val {attr}\"):\n","                    images, labels = images.to(device), labels.to(device)\n","                    outputs = model(images, attr_index=attr_index)\n","                    val_loss += criterion(outputs, labels).item()\n","\n","                    # Store predictions and labels for metrics\n","                    _, preds = torch.max(outputs, 1)\n","                    all_val_preds.extend(preds.cpu().numpy())\n","                    all_val_labels.extend(labels.cpu().numpy())\n","\n","            # Calculate validation metrics\n","            val_accuracy = accuracy_score(all_val_labels, all_val_preds)\n","            val_precision = precision_score(all_val_labels, all_val_preds, average='weighted')\n","            val_recall = recall_score(all_val_labels, all_val_preds, average='weighted')\n","            val_f1 = f1_score(all_val_labels, all_val_preds, average='weighted')\n","\n","            # Save model weights if validation accuracy improves for this attribute\n","            if val_accuracy >= best_val_acc:\n","                best_val_acc = val_accuracy\n","                # Save the model's state dictionary using .module\n","                torch.save(model.module.state_dict(), f'best_model_attr_{model_type}.pth')\n","\n","\n","            # Print metrics for the epoch\n","            print(f\"Epoch {epoch+1}/{num_epochs}, Attr: {attr}\")\n","            print(f\"Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n","                  f\"Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1-score: {train_f1:.4f}\")\n","            print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.4f}, \"\n","                  f\"Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1-score: {val_f1:.4f}\")\n","    torch.save(model.module.state_dict(), f'best_model_attr_end_{model_type}.pth')"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-11-09T06:13:56.666932Z","iopub.status.busy":"2024-11-09T06:13:56.666596Z","iopub.status.idle":"2024-11-09T06:13:56.678289Z","shell.execute_reply":"2024-11-09T06:13:56.677368Z","shell.execute_reply.started":"2024-11-09T06:13:56.666891Z"},"trusted":true},"outputs":[],"source":["import pickle\n","import gc\n","\n","def main(test_c_name):\n","    # Set image directory\n","    image_dir = f'{input_path}/train_images'\n","    # Initialize device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    # Define a function to train each model sequentially\n","    def train_single_model(data1,data2, num_attr_columns, model_type):\n","        attr_columns = [f'attr_{i}' for i in range(1, num_attr_columns+1)]\n","        print(f\"Preparing data for {model_type}\")\n","        label_encoders = LabelEncoderDict()\n","        label_encoders.fit(data2, attr_columns)\n","    \n","        train_loader, val_loader, label_encoders, num_classes_per_attr = prepare_data(data1,label_encoders, image_dir, batch_size=4, num_attr_columns=num_attr_columns)\n","        world_size = torch.cuda.device_count()\n","        # Initialize the model\n","        print(f\"Initializing model {model_type}\")\n","        model = MultiLabelClassifier(semi_classes_dict[test_c_name]).to(device)\n","        # Train the model\n","        print(f\"Training model with base model {model_type}\")\n","        model = train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCH, num_classes_per_attr=num_classes_per_attr, model_type=model_type)\n","        print(f\"\\nTraining model with attribute only {model_type}\")\n","        attr_loaders = prepare_attribute_data(data2,image_dir,label_encoders, batch_size=4,num_attr_columns=num_attr_columns)\n","        train_attribute_model(model,attr_loaders,NUM_ATTR_EPOCHS,model_type)\n","        # Save label encoders\n","        with open(f'label_encoders_{model_type}.pkl', 'wb') as f:\n","            pickle.dump(label_encoders, f)\n","        print(\"---------------------------------------------\")\n","        print(f\"number of classes for {model_type} is {num_classes_per_attr}\")\n","        print(\"---------------------------------------------\")\n","        # Free up memory\n","        del model, train_loader, val_loader, label_encoders, num_classes_per_attr\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","    df1,df2 = do_preprocessing(test_c_name, category_dict[test_c_name])\n","    train_single_model(df1,df2, num_attr_columns=len(semi_classes_dict[test_c_name]), model_type=test_c_name)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-11-09T06:13:56.679766Z","iopub.status.busy":"2024-11-09T06:13:56.679470Z","iopub.status.idle":"2024-11-09T06:18:17.448418Z","shell.execute_reply":"2024-11-09T06:18:17.445360Z","shell.execute_reply.started":"2024-11-09T06:13:56.679734Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of features for original df: [7, 3, 3, 3, 6, 3, 2, 2]\n","\n","Number of features for new df: [7, 2, 2, 1, 5, 2, 1, 2]\n","Preparing data for c4\n","Initializing model c4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"770655e24b1f431a83dce28accb41710","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/69.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a5d177583ef4cbd9a71dd5f3623e1bf","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/354M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Training model with base model c4\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/4 [Training]: 100%|██████████| 58/58 [00:53<00:00,  1.08batch/s, loss=0.556]\n","Epoch 1/4 [Validation]: 100%|██████████| 6/6 [00:01<00:00,  4.30batch/s, loss=0.495]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/4\n","Training Loss: 0.7592\n","Validation Loss: 0.4567\n","Attribute 1 - Train Acc: 51.72%, Val Acc: 66.67%\n","Attribute 1 - Train Precision: 0.32, Train Recall: 0.52, Train F1-Score: 0.39\n","Attribute 1 - Val Precision: 0.44, Val Recall: 0.67, Val F1-Score: 0.53\n","\n","Attribute 2 - Train Acc: 93.10%, Val Acc: 95.83%\n","Attribute 2 - Train Precision: 0.96, Train Recall: 0.93, Train F1-Score: 0.95\n","Attribute 2 - Val Precision: 0.92, Val Recall: 0.96, Val F1-Score: 0.94\n","\n","Attribute 3 - Train Acc: 59.91%, Val Acc: 70.83%\n","Attribute 3 - Train Precision: 0.57, Train Recall: 0.60, Train F1-Score: 0.58\n","Attribute 3 - Val Precision: 0.50, Val Recall: 0.71, Val F1-Score: 0.59\n","\n","Attribute 4 - Train Acc: 87.50%, Val Acc: 100.00%\n","Attribute 4 - Train Precision: 1.00, Train Recall: 0.88, Train F1-Score: 0.93\n","Attribute 4 - Val Precision: 1.00, Val Recall: 1.00, Val F1-Score: 1.00\n","\n","Attribute 5 - Train Acc: 54.31%, Val Acc: 70.83%\n","Attribute 5 - Train Precision: 0.45, Train Recall: 0.54, Train F1-Score: 0.49\n","Attribute 5 - Val Precision: 0.50, Val Recall: 0.71, Val F1-Score: 0.59\n","\n","Attribute 6 - Train Acc: 83.62%, Val Acc: 95.83%\n","Attribute 6 - Train Precision: 1.00, Train Recall: 0.84, Train F1-Score: 0.91\n","Attribute 6 - Val Precision: 0.92, Val Recall: 0.96, Val F1-Score: 0.94\n","\n","Attribute 7 - Train Acc: 82.76%, Val Acc: 100.00%\n","Attribute 7 - Train Precision: 1.00, Train Recall: 0.83, Train F1-Score: 0.91\n","Attribute 7 - Val Precision: 1.00, Val Recall: 1.00, Val F1-Score: 1.00\n","\n","Attribute 8 - Train Acc: 98.71%, Val Acc: 100.00%\n","Attribute 8 - Train Precision: 0.97, Train Recall: 0.99, Train F1-Score: 0.98\n","Attribute 8 - Val Precision: 1.00, Val Recall: 1.00, Val F1-Score: 1.00\n","\n","Overall Train Accuracy: 6.90%\n","Overall Validation Accuracy: 16.67%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/4 [Training]: 100%|██████████| 58/58 [00:52<00:00,  1.11batch/s, loss=0.441]\n","Epoch 2/4 [Validation]: 100%|██████████| 6/6 [00:01<00:00,  3.83batch/s, loss=0.416]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/4\n","Training Loss: 0.4365\n","Validation Loss: 0.3579\n","Attribute 1 - Train Acc: 57.76%, Val Acc: 66.67%\n","Attribute 1 - Train Precision: 0.54, Train Recall: 0.58, Train F1-Score: 0.44\n","Attribute 1 - Val Precision: 0.44, Val Recall: 0.67, Val F1-Score: 0.53\n","\n","Attribute 2 - Train Acc: 98.28%, Val Acc: 95.83%\n","Attribute 2 - Train Precision: 0.97, Train Recall: 0.98, Train F1-Score: 0.97\n","Attribute 2 - Val Precision: 0.92, Val Recall: 0.96, Val F1-Score: 0.94\n","\n","Attribute 3 - Train Acc: 76.72%, Val Acc: 95.83%\n","Attribute 3 - Train Precision: 0.82, Train Recall: 0.77, Train F1-Score: 0.70\n","Attribute 3 - Val Precision: 0.96, Val Recall: 0.96, Val F1-Score: 0.96\n","\n","Attribute 4 - Train Acc: 100.00%, Val Acc: 100.00%\n","Attribute 4 - Train Precision: 1.00, Train Recall: 1.00, Train F1-Score: 1.00\n","Attribute 4 - Val Precision: 1.00, Val Recall: 1.00, Val F1-Score: 1.00\n","\n","Attribute 5 - Train Acc: 66.81%, Val Acc: 70.83%\n","Attribute 5 - Train Precision: 0.45, Train Recall: 0.67, Train F1-Score: 0.54\n","Attribute 5 - Val Precision: 0.50, Val Recall: 0.71, Val F1-Score: 0.59\n","\n","Attribute 6 - Train Acc: 100.00%, Val Acc: 95.83%\n","Attribute 6 - Train Precision: 1.00, Train Recall: 1.00, Train F1-Score: 1.00\n","Attribute 6 - Val Precision: 0.92, Val Recall: 0.96, Val F1-Score: 0.94\n","\n","Attribute 7 - Train Acc: 100.00%, Val Acc: 100.00%\n","Attribute 7 - Train Precision: 1.00, Train Recall: 1.00, Train F1-Score: 1.00\n","Attribute 7 - Val Precision: 1.00, Val Recall: 1.00, Val F1-Score: 1.00\n","\n","Attribute 8 - Train Acc: 98.71%, Val Acc: 100.00%\n","Attribute 8 - Train Precision: 0.97, Train Recall: 0.99, Train F1-Score: 0.98\n","Attribute 8 - Val Precision: 1.00, Val Recall: 1.00, Val F1-Score: 1.00\n","\n","Overall Train Accuracy: 13.79%\n","Overall Validation Accuracy: 29.17%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/4 [Training]: 100%|██████████| 58/58 [00:53<00:00,  1.08batch/s, loss=0.309]\n","Epoch 3/4 [Validation]: 100%|██████████| 6/6 [00:01<00:00,  4.59batch/s, loss=0.367]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/4\n","Training Loss: 0.3498\n","Validation Loss: 0.2940\n","Attribute 1 - Train Acc: 70.26%, Val Acc: 75.00%\n","Attribute 1 - Train Precision: 0.55, Train Recall: 0.70, Train F1-Score: 0.61\n","Attribute 1 - Val Precision: 0.61, Val Recall: 0.75, Val F1-Score: 0.67\n","\n","Attribute 2 - Train Acc: 98.28%, Val Acc: 95.83%\n","Attribute 2 - Train Precision: 0.97, Train Recall: 0.98, Train F1-Score: 0.97\n","Attribute 2 - Val Precision: 0.92, Val Recall: 0.96, Val F1-Score: 0.94\n","\n","Attribute 3 - Train Acc: 92.24%, Val Acc: 100.00%\n","Attribute 3 - Train Precision: 0.92, Train Recall: 0.92, Train F1-Score: 0.92\n","Attribute 3 - Val Precision: 1.00, Val Recall: 1.00, Val F1-Score: 1.00\n","\n","Attribute 4 - Train Acc: 100.00%, Val Acc: 100.00%\n","Attribute 4 - Train Precision: 1.00, Train Recall: 1.00, Train F1-Score: 1.00\n","Attribute 4 - Val Precision: 1.00, Val Recall: 1.00, Val F1-Score: 1.00\n","\n","Attribute 5 - Train Acc: 66.81%, Val Acc: 70.83%\n","Attribute 5 - Train Precision: 0.45, Train Recall: 0.67, Train F1-Score: 0.54\n","Attribute 5 - Val Precision: 0.50, Val Recall: 0.71, Val F1-Score: 0.59\n","\n","Attribute 6 - Train Acc: 100.00%, Val Acc: 95.83%\n","Attribute 6 - Train Precision: 1.00, Train Recall: 1.00, Train F1-Score: 1.00\n","Attribute 6 - Val Precision: 0.92, Val Recall: 0.96, Val F1-Score: 0.94\n","\n","Attribute 7 - Train Acc: 100.00%, Val Acc: 100.00%\n","Attribute 7 - Train Precision: 1.00, Train Recall: 1.00, Train F1-Score: 1.00\n","Attribute 7 - Val Precision: 1.00, Val Recall: 1.00, Val F1-Score: 1.00\n","\n","Attribute 8 - Train Acc: 98.71%, Val Acc: 100.00%\n","Attribute 8 - Train Precision: 0.97, Train Recall: 0.99, Train F1-Score: 0.98\n","Attribute 8 - Val Precision: 1.00, Val Recall: 1.00, Val F1-Score: 1.00\n","\n","Overall Train Accuracy: 30.17%\n","Overall Validation Accuracy: 37.50%\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/4 [Training]: 100%|██████████| 58/58 [00:53<00:00,  1.09batch/s, loss=0.163]\n","Epoch 4/4 [Validation]: 100%|██████████| 6/6 [00:01<00:00,  4.59batch/s, loss=0.322]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/4\n","Training Loss: 0.2900\n","Validation Loss: 0.2592\n","Attribute 1 - Train Acc: 75.86%, Val Acc: 79.17%\n","Attribute 1 - Train Precision: 0.61, Train Recall: 0.76, Train F1-Score: 0.67\n","Attribute 1 - Val Precision: 0.71, Val Recall: 0.79, Val F1-Score: 0.73\n","\n","Attribute 2 - Train Acc: 98.28%, Val Acc: 95.83%\n","Attribute 2 - Train Precision: 0.97, Train Recall: 0.98, Train F1-Score: 0.97\n","Attribute 2 - Val Precision: 0.92, Val Recall: 0.96, Val F1-Score: 0.94\n","\n","Attribute 3 - Train Acc: 96.12%, Val Acc: 100.00%\n","Attribute 3 - Train Precision: 0.96, Train Recall: 0.96, Train F1-Score: 0.96\n","Attribute 3 - Val Precision: 1.00, Val Recall: 1.00, Val F1-Score: 1.00\n","\n","Attribute 4 - Train Acc: 100.00%, Val Acc: 100.00%\n","Attribute 4 - Train Precision: 1.00, Train Recall: 1.00, Train F1-Score: 1.00\n","Attribute 4 - Val Precision: 1.00, Val Recall: 1.00, Val F1-Score: 1.00\n","\n","Attribute 5 - Train Acc: 66.81%, Val Acc: 70.83%\n","Attribute 5 - Train Precision: 0.45, Train Recall: 0.67, Train F1-Score: 0.54\n","Attribute 5 - Val Precision: 0.50, Val Recall: 0.71, Val F1-Score: 0.59\n","\n","Attribute 6 - Train Acc: 100.00%, Val Acc: 95.83%\n","Attribute 6 - Train Precision: 1.00, Train Recall: 1.00, Train F1-Score: 1.00\n","Attribute 6 - Val Precision: 0.92, Val Recall: 0.96, Val F1-Score: 0.94\n","\n","Attribute 7 - Train Acc: 100.00%, Val Acc: 100.00%\n","Attribute 7 - Train Precision: 1.00, Train Recall: 1.00, Train F1-Score: 1.00\n","Attribute 7 - Val Precision: 1.00, Val Recall: 1.00, Val F1-Score: 1.00\n","\n","Attribute 8 - Train Acc: 98.71%, Val Acc: 100.00%\n","Attribute 8 - Train Precision: 0.97, Train Recall: 0.99, Train F1-Score: 0.98\n","Attribute 8 - Val Precision: 1.00, Val Recall: 1.00, Val F1-Score: 1.00\n","\n","Overall Train Accuracy: 37.07%\n","Overall Validation Accuracy: 41.67%\n","\n","Training model with attribute only c4\n","attr_1: 15556 train samples, 1729 val samples\n","attr_2: 13320 train samples, 1481 val samples\n","attr_3: 14922 train samples, 1658 val samples\n","attr_4: 14544 train samples, 1617 val samples\n","attr_5: 15330 train samples, 1704 val samples\n","attr_6: 14436 train samples, 1605 val samples\n","attr_7: 13914 train samples, 1546 val samples\n","attr_8: 459 train samples, 51 val samples\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1 train attr_1:   2%|▏         | 94/3889 [00:29<19:43,  3.21it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_c_name\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[12], line 39\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(test_c_name)\u001b[0m\n\u001b[1;32m     36\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     38\u001b[0m df1,df2 \u001b[38;5;241m=\u001b[39m do_preprocessing(test_c_name, category_dict[test_c_name])\n\u001b[0;32m---> 39\u001b[0m \u001b[43mtrain_single_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_attr_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msemi_classes_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_c_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_c_name\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[12], line 26\u001b[0m, in \u001b[0;36mmain.<locals>.train_single_model\u001b[0;34m(data1, data2, num_attr_columns, model_type)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining model with attribute only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m attr_loaders \u001b[38;5;241m=\u001b[39m prepare_attribute_data(data2,image_dir,label_encoders, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,num_attr_columns\u001b[38;5;241m=\u001b[39mnum_attr_columns)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrain_attribute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattr_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNUM_ATTR_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Save label encoders\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_encoders_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n","Cell \u001b[0;32mIn[11], line 37\u001b[0m, in \u001b[0;36mtrain_attribute_model\u001b[0;34m(model, dataloaders, num_epochs, model_type, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images, attr_index\u001b[38;5;241m=\u001b[39mattr_index)\n\u001b[1;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 37\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     40\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["main(test_c_name)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-11-09T05:45:12.979885Z","iopub.status.busy":"2024-11-09T05:45:12.979565Z","iopub.status.idle":"2024-11-09T05:45:13.004318Z","shell.execute_reply":"2024-11-09T05:45:13.003395Z","shell.execute_reply.started":"2024-11-09T05:45:12.979854Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>13349</th>\n","      <td>13615</td>\n","      <td>Women Tshirts</td>\n","    </tr>\n","    <tr>\n","      <th>13350</th>\n","      <td>13616</td>\n","      <td>Women Tshirts</td>\n","    </tr>\n","    <tr>\n","      <th>13351</th>\n","      <td>13617</td>\n","      <td>Women Tshirts</td>\n","    </tr>\n","    <tr>\n","      <th>13352</th>\n","      <td>13618</td>\n","      <td>Women Tshirts</td>\n","    </tr>\n","    <tr>\n","      <th>13353</th>\n","      <td>13619</td>\n","      <td>Women Tshirts</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>23275</th>\n","      <td>23559</td>\n","      <td>Women Tshirts</td>\n","    </tr>\n","    <tr>\n","      <th>23276</th>\n","      <td>23560</td>\n","      <td>Women Tshirts</td>\n","    </tr>\n","    <tr>\n","      <th>23277</th>\n","      <td>23561</td>\n","      <td>Women Tshirts</td>\n","    </tr>\n","    <tr>\n","      <th>23278</th>\n","      <td>23562</td>\n","      <td>Women Tshirts</td>\n","    </tr>\n","    <tr>\n","      <th>23279</th>\n","      <td>23563</td>\n","      <td>Women Tshirts</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9931 rows × 2 columns</p>\n","</div>"],"text/plain":["          id       Category\n","13349  13615  Women Tshirts\n","13350  13616  Women Tshirts\n","13351  13617  Women Tshirts\n","13352  13618  Women Tshirts\n","13353  13619  Women Tshirts\n","...      ...            ...\n","23275  23559  Women Tshirts\n","23276  23560  Women Tshirts\n","23277  23561  Women Tshirts\n","23278  23562  Women Tshirts\n","23279  23563  Women Tshirts\n","\n","[9931 rows x 2 columns]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["test_df_semi = df_test[df_test['Category'] == category_dict[test_c_name]]\n","test_df_semi"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-11-09T05:45:13.005825Z","iopub.status.busy":"2024-11-09T05:45:13.005523Z","iopub.status.idle":"2024-11-09T05:45:13.018505Z","shell.execute_reply":"2024-11-09T05:45:13.017750Z","shell.execute_reply.started":"2024-11-09T05:45:13.005790Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torchvision import transforms\n","from PIL import Image\n","import pickle\n","from tqdm import tqdm\n","import time\n","\n","def load_model(model_path, num_classes_per_attr, device):\n","    # Initialize the model architecture and load the saved weights\n","    model = MultiLabelClassifier(num_classes_per_attr)\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model = torch.nn.DataParallel(model)\n","    model.to(device)\n","    model.eval()\n","    return model\n","\n","def load_label_encoders(encoder_path):\n","    with open(encoder_path, 'rb') as f:\n","        encoders = pickle.load(f)\n","    return encoders\n","\n","def preprocess_image(image_path, image_size=(512,512)):\n","    # Define image transformations (same as used during training)\n","    transform = transforms.Compose([\n","        transforms.Resize((512,512)),  # TODO: Change with (512, 512)\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])\n","    ])\n","    \n","    # Open image and apply transformations\n","    image = Image.open(image_path).convert('RGB')\n","    image = transform(image)\n","    image = image.unsqueeze(0)  # Add batch dimension\n","    return image\n","\n","def inference(images, model, label_encoders):\n","    # Perform inference\n","    with torch.no_grad():\n","        outputs = model(x=images,return_features=False)\n","\n","    # Decode predictions\n","    predicted_labels = []\n","    for i, output in enumerate(outputs):\n","        _, predicted = torch.max(output, 1)\n","\n","        attr_name = f'attr_{i + 1}'\n","        if attr_name in label_encoders.encoders:\n","            decoded_label = label_encoders.encoders[attr_name].inverse_transform([predicted.item()])[0]\n","            predicted_labels.append(decoded_label)\n","        else:\n","            raise KeyError(f\"Encoder for {attr_name} not found in the loaded label encoders.\")\n","    \n","    return predicted_labels"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-11-09T05:45:13.019963Z","iopub.status.busy":"2024-11-09T05:45:13.019621Z","iopub.status.idle":"2024-11-09T05:46:43.891447Z","shell.execute_reply":"2024-11-09T05:46:43.888360Z","shell.execute_reply.started":"2024-11-09T05:45:13.019924Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing Images:   9%|▊         | 859/9931 [01:28<15:36,  9.69it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(val)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m6\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m image \u001b[38;5;241m=\u001b[39m preprocess_image(image_path)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Preprocess and send image to device\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_encoders\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Use the already loaded model and encoders\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, interval):\n\u001b[1;32m     17\u001b[0m     preds[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(predictions[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n","Cell \u001b[0;32mIn[15], line 49\u001b[0m, in \u001b[0;36minference\u001b[0;34m(images, model, label_encoders)\u001b[0m\n\u001b[1;32m     47\u001b[0m attr_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr_name \u001b[38;5;129;01min\u001b[39;00m label_encoders\u001b[38;5;241m.\u001b[39mencoders:\n\u001b[0;32m---> 49\u001b[0m     decoded_label \u001b[38;5;241m=\u001b[39m label_encoders\u001b[38;5;241m.\u001b[39mencoders[attr_name]\u001b[38;5;241m.\u001b[39minverse_transform([\u001b[43mpredicted\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     50\u001b[0m     predicted_labels\u001b[38;5;241m.\u001b[39mappend(decoded_label)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model_path = f\"best_model_{test_c_name}.pth\"\n","encoder_path = f\"{working_path}/label_encoders_{test_c_name}.pkl\"\n","num_classes_per_attr = NUM_OF_SEMI_CLASSES_OF_COLUMNS\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = load_model(model_path, num_classes_per_attr, device)\n","label_encoders = load_label_encoders(encoder_path)\n","image_dir = f\"{input_path}/test_images\"\n","interval = len(semi_classes_dict[test_c_name]) +1\n","preds = {f'attr_{i}': [] for i in range(1,interval )}\n","t1 = time.time()\n","\n","for val in tqdm(test_df_semi['id'], desc='Processing Images', total=len(test_df_semi)):\n","    image_path = f\"{image_dir}/{str(val).zfill(6)}.jpg\"\n","    image = preprocess_image(image_path).to(device)  # Preprocess and send image to device\n","    predictions = inference(image, model, label_encoders)  # Use the already loaded model and encoders\n","    for i in range(1, interval):\n","        preds[f'attr_{i}'].append(predictions[i-1])\n","print(f'Time taken to process images is {time.time() - t1} seconds, which is {len(test_df_semi) / (time.time() - t1)} images per second')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-11-09T05:46:43.892412Z","iopub.status.idle":"2024-11-09T05:46:43.892797Z","shell.execute_reply":"2024-11-09T05:46:43.892630Z","shell.execute_reply.started":"2024-11-09T05:46:43.892610Z"},"trusted":true},"outputs":[],"source":["for i in range(1,interval):\n","    test_df_semi[f'attr_{i}'] = preds[f'attr_{i}']\n","test_df_semi    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-11-09T05:46:43.894077Z","iopub.status.idle":"2024-11-09T05:46:43.894451Z","shell.execute_reply":"2024-11-09T05:46:43.894287Z","shell.execute_reply.started":"2024-11-09T05:46:43.894262Z"},"trusted":true},"outputs":[],"source":["test_df_semi.to_csv(f'test_validation_df_{test_c_name}.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-11-09T05:46:43.895926Z","iopub.status.idle":"2024-11-09T05:46:43.896401Z","shell.execute_reply":"2024-11-09T05:46:43.896203Z","shell.execute_reply.started":"2024-11-09T05:46:43.896183Z"},"trusted":true},"outputs":[],"source":["model_path = f\"best_model_end_{test_c_name}.pth\"\n","encoder_path = f\"{working_path}/label_encoders_{test_c_name}.pkl\"\n","num_classes_per_attr = NUM_OF_SEMI_CLASSES_OF_COLUMNS\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = load_model(model_path, num_classes_per_attr, device)\n","label_encoders = load_label_encoders(encoder_path)\n","image_dir = f\"{input_path}/test_images\"\n","preds = {f'attr_{i}': [] for i in range(1, interval)}\n","t1 = time.time()\n","for val in tqdm(test_df_semi['id'], desc='Processing Images', total=len(test_df_semi)):\n","    image_path = f\"{image_dir}/{str(val).zfill(6)}.jpg\"\n","    image = preprocess_image(image_path).to(device)  # Preprocess and send image to device\n","    predictions = inference(image, model, label_encoders)  # Use the already loaded model and encoders\n","    for i in range(1, interval):\n","        preds[f'attr_{i}'].append(predictions[i-1])\n","print(f'Time taken to process images is {time.time() - t1} seconds, which is {len(test_df_semi) / (time.time() - t1)} images per second')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-11-09T05:46:43.897980Z","iopub.status.idle":"2024-11-09T05:46:43.898385Z","shell.execute_reply":"2024-11-09T05:46:43.898221Z","shell.execute_reply.started":"2024-11-09T05:46:43.898190Z"},"trusted":true},"outputs":[],"source":["for i in range(1,interval):\n","    test_df_semi[f'attr_{i}'] = preds[f'attr_{i}']\n","test_df_semi    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-11-09T05:46:43.899729Z","iopub.status.idle":"2024-11-09T05:46:43.900100Z","shell.execute_reply":"2024-11-09T05:46:43.899915Z","shell.execute_reply.started":"2024-11-09T05:46:43.899898Z"},"trusted":true},"outputs":[],"source":["test_df_semi.to_csv(f'test_df_semi_{test_c_name}.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-11-09T05:46:43.901793Z","iopub.status.idle":"2024-11-09T05:46:43.902187Z","shell.execute_reply":"2024-11-09T05:46:43.901994Z","shell.execute_reply.started":"2024-11-09T05:46:43.901975Z"},"trusted":true},"outputs":[],"source":["model_path = f'best_model_attr_{test_c_name}.pth'\n","encoder_path = f\"{working_path}/label_encoders_{test_c_name}.pkl\"\n","num_classes_per_attr = NUM_OF_SEMI_CLASSES_OF_COLUMNS\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = load_model(model_path, num_classes_per_attr, device)\n","label_encoders = load_label_encoders(encoder_path)\n","image_dir = f\"{input_path}/test_images\"\n","preds = {f'attr_{i}': [] for i in range(1, interval)}\n","t1 = time.time()\n","for val in tqdm(test_df_semi['id'], desc='Processing Images', total=len(test_df_semi)):\n","    image_path = f\"{image_dir}/{str(val).zfill(6)}.jpg\"\n","    image = preprocess_image(image_path).to(device)  # Preprocess and send image to device\n","    predictions = inference(image, model, label_encoders)  # Use the already loaded model and encoders\n","    for i in range(1, interval):\n","        preds[f'attr_{i}'].append(predictions[i-1])\n","print(f'Time taken to process images is {time.time() - t1} seconds, which is {len(test_df_semi) / (time.time() - t1)} images per second')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-11-09T05:46:43.903586Z","iopub.status.idle":"2024-11-09T05:46:43.903919Z","shell.execute_reply":"2024-11-09T05:46:43.903764Z","shell.execute_reply.started":"2024-11-09T05:46:43.903747Z"},"trusted":true},"outputs":[],"source":["for i in range(1,interval):\n","    test_df_semi[f'attr_{i}'] = preds[f'attr_{i}']\n","test_df_semi    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-11-09T05:46:43.905273Z","iopub.status.idle":"2024-11-09T05:46:43.905656Z","shell.execute_reply":"2024-11-09T05:46:43.905486Z","shell.execute_reply.started":"2024-11-09T05:46:43.905467Z"},"trusted":true},"outputs":[],"source":["test_df_semi.to_csv(f'test_attr_validation_df_{test_c_name}.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-11-09T05:46:43.906588Z","iopub.status.idle":"2024-11-09T05:46:43.906947Z","shell.execute_reply":"2024-11-09T05:46:43.906783Z","shell.execute_reply.started":"2024-11-09T05:46:43.906764Z"},"trusted":true},"outputs":[],"source":["model_path = f'best_model_attr_end_{test_c_name}.pth'\n","encoder_path = f\"{working_path}/label_encoders_{test_c_name}.pkl\"\n","num_classes_per_attr = NUM_OF_SEMI_CLASSES_OF_COLUMNS\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = load_model(model_path, num_classes_per_attr, device)\n","label_encoders = load_label_encoders(encoder_path)\n","image_dir = f\"{input_path}/test_images\"\n","preds = {f'attr_{i}': [] for i in range(1, interval)}\n","t1 = time.time()\n","for val in tqdm(test_df_semi['id'], desc='Processing Images', total=len(test_df_semi)):\n","    image_path = f\"{image_dir}/{str(val).zfill(6)}.jpg\"\n","    image = preprocess_image(image_path).to(device)  # Preprocess and send image to device\n","    predictions = inference(image, model, label_encoders)  # Use the already loaded model and encoders\n","    for i in range(1, interval):\n","        preds[f'attr_{i}'].append(predictions[i-1])\n","print(f'Time taken to process images is {time.time() - t1} seconds, which is {len(test_df_semi) / (time.time() - t1)} images per second')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-11-09T05:46:43.908065Z","iopub.status.idle":"2024-11-09T05:46:43.908428Z","shell.execute_reply":"2024-11-09T05:46:43.908263Z","shell.execute_reply.started":"2024-11-09T05:46:43.908244Z"},"trusted":true},"outputs":[],"source":["for i in range(1,interval):\n","    test_df_semi[f'attr_{i}'] = preds[f'attr_{i}']\n","test_df_semi    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-11-09T05:46:43.910094Z","iopub.status.idle":"2024-11-09T05:46:43.910443Z","shell.execute_reply":"2024-11-09T05:46:43.910285Z","shell.execute_reply.started":"2024-11-09T05:46:43.910266Z"},"trusted":true},"outputs":[],"source":["test_df_semi.to_csv(f'test_df_attr_semi_{test_c_name}.csv',index=False)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5923426,"sourceId":9689319,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
