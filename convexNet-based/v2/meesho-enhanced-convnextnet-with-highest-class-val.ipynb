{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84705,"databundleVersionId":9755748,"sourceType":"competition"},{"sourceId":9689319,"sourceType":"datasetVersion","datasetId":5923426}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":39437.879367,"end_time":"2024-10-29T06:34:38.252320","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-28T19:37:20.372953","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"11864aaf32994b65b89dc3ed4f968c9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f0eab7cb4fa4724832342ce31e60c23","placeholder":"​","style":"IPY_MODEL_5b8fd9650b514960bfe6b4158aee3863","value":"pytorch_model.bin: 100%"}},"149e1c9a5d6e4cfca3742db062545dac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1581d846ca014f278d1ae56c4567349a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1674cef0064a47b8a4da0ecf7de68dd9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae72ecf325784199bd9c78981ac9eb23","max":354492753,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7447cbd8719486b8494515350a7246d","value":354492753}},"22f6bb458378444b8cc9ae49acc53fcc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f356a9e8f78e4953963a6b46e591b832","placeholder":"​","style":"IPY_MODEL_149e1c9a5d6e4cfca3742db062545dac","value":" 354M/354M [00:01&lt;00:00, 245MB/s]"}},"28f3a7b9eabc4e2686e0e058a375c4df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33367af0a4d24307b77e282ccf49006d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1581d846ca014f278d1ae56c4567349a","placeholder":"​","style":"IPY_MODEL_62607cdf08d24db4bded6f463b93454a","value":"config.json: 100%"}},"557a42d7af434a02a71fce7b58e0adb8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9708adad26d460892d7f84f977678bb","max":69643,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d6ccbe61028240a69731512338121173","value":69643}},"57df8a0ed1ea4a0789213ceba10d6585":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11864aaf32994b65b89dc3ed4f968c9d","IPY_MODEL_1674cef0064a47b8a4da0ecf7de68dd9","IPY_MODEL_22f6bb458378444b8cc9ae49acc53fcc"],"layout":"IPY_MODEL_6f7efe6deda243f2b6166d587770145e"}},"5b8fd9650b514960bfe6b4158aee3863":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62607cdf08d24db4bded6f463b93454a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f7efe6deda243f2b6166d587770145e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74809382d2554a5590443e0291470bb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33367af0a4d24307b77e282ccf49006d","IPY_MODEL_557a42d7af434a02a71fce7b58e0adb8","IPY_MODEL_b2b353118a7349a88c70f5ca53647b9c"],"layout":"IPY_MODEL_9a20af6ee68244ddb01b0ced701e827e"}},"8f0eab7cb4fa4724832342ce31e60c23":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a20af6ee68244ddb01b0ced701e827e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9aba1d7b2424c8e881ee12ef5c34e73":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae72ecf325784199bd9c78981ac9eb23":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2b353118a7349a88c70f5ca53647b9c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9aba1d7b2424c8e881ee12ef5c34e73","placeholder":"​","style":"IPY_MODEL_28f3a7b9eabc4e2686e0e058a375c4df","value":" 69.6k/69.6k [00:00&lt;00:00, 2.45MB/s]"}},"c9708adad26d460892d7f84f977678bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6ccbe61028240a69731512338121173":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f356a9e8f78e4953963a6b46e591b832":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7447cbd8719486b8494515350a7246d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"412690dd","cell_type":"code","source":"import pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom PIL import Image\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\nimport torch.nn.functional as F\nimport random","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-11-07T19:16:04.718935Z","iopub.execute_input":"2024-11-07T19:16:04.719849Z","iopub.status.idle":"2024-11-07T19:16:04.725956Z","shell.execute_reply.started":"2024-11-07T19:16:04.719804Z","shell.execute_reply":"2024-11-07T19:16:04.724849Z"},"papermill":{"duration":7.972482,"end_time":"2024-10-28T19:37:31.376772","exception":false,"start_time":"2024-10-28T19:37:23.404290","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":4},{"id":"bb0b8656","cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# pre-defined\ncategory_dict = {'c1':'Men Tshirts', 'c2':'Sarees', 'c3': 'Kurtis', 'c4': 'Women Tshirts', 'c5': 'Women Tops & Tunics'}\nsemi_classes_dict = {'c1':[4, 2, 2, 3, 2], 'c2':[4, 6, 3, 8, 4, 3, 4, 5, 9, 2], 'c3': [13, 2, 2, 2, 2, 2, 2, 3, 2], 'c4': [7, 3, 3, 3, 6, 3, 2, 2], 'c5': [12, 4, 2, 7, 2, 3, 6, 4, 4, 6]}","metadata":{"execution":{"iopub.status.busy":"2024-11-07T19:16:04.730182Z","iopub.execute_input":"2024-11-07T19:16:04.730622Z","iopub.status.idle":"2024-11-07T19:16:04.737587Z","shell.execute_reply.started":"2024-11-07T19:16:04.730579Z","shell.execute_reply":"2024-11-07T19:16:04.736632Z"},"papermill":{"duration":0.012999,"end_time":"2024-10-28T19:37:31.396211","exception":false,"start_time":"2024-10-28T19:37:31.383212","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":5},{"id":"dd7ce95d-8cbb-45d2-a01c-681f5570fdff","cell_type":"code","source":"# set paths as per the set-up and Hyperparameters\ninput_path = \"/kaggle/input/visual-taxonomy\"\nworking_path = \"/kaggle/working\"\ntest_c_name = \"c3\"\nNUM_EPOCH = 40\nLEARNING_RATE = 0.001\nNUM_OF_SEMI_CLASSES_OF_COLUMNS = semi_classes_dict[test_c_name]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T19:16:04.739345Z","iopub.execute_input":"2024-11-07T19:16:04.739731Z","iopub.status.idle":"2024-11-07T19:16:04.746702Z","shell.execute_reply.started":"2024-11-07T19:16:04.739695Z","shell.execute_reply":"2024-11-07T19:16:04.745818Z"}},"outputs":[],"execution_count":6},{"id":"07e7ce2a","cell_type":"code","source":"df_train = pd.read_csv(f'{input_path}/train.csv')\ndf_test = pd.read_csv(f'{input_path}/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-11-07T19:16:04.747919Z","iopub.execute_input":"2024-11-07T19:16:04.748633Z","iopub.status.idle":"2024-11-07T19:16:04.987480Z","shell.execute_reply.started":"2024-11-07T19:16:04.748591Z","shell.execute_reply":"2024-11-07T19:16:04.986456Z"},"papermill":{"duration":0.259368,"end_time":"2024-10-28T19:37:31.661824","exception":false,"start_time":"2024-10-28T19:37:31.402456","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":7},{"id":"47b46533-b45d-44bc-991d-e428de2332b8","cell_type":"code","source":"def replace_na_values_with_highest_class(df_obj):\n\n    # Select columns from 'attr_1' to the end of the dataframe\n    attr_columns = df_obj.loc[:, 'attr_1':]\n    \n    # Replace NaN values with the most frequent class in each column\n    for col in attr_columns.columns:\n        # Find the most frequent value in the column\n        highest_class = attr_columns[col].value_counts().idxmax()\n        # Replace NaN values in the column with the most frequent value\n        df_obj[col].fillna(highest_class, inplace=True)\n\n    return df_obj","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T19:16:04.990452Z","iopub.execute_input":"2024-11-07T19:16:04.990856Z","iopub.status.idle":"2024-11-07T19:16:04.996562Z","shell.execute_reply.started":"2024-11-07T19:16:04.990812Z","shell.execute_reply":"2024-11-07T19:16:04.995693Z"}},"outputs":[],"execution_count":8},{"id":"aad59d1f-726d-4ecd-bfd2-1b4f28778552","cell_type":"code","source":"def do_preprocessing(test_c_name, test_category):\n    df_sub = df_train[df_train['Category'] == test_category]\n    print(df_sub.info())\n    df_sub.dropna(axis=1, how='all', inplace=True)\n    df_sub = replace_na_values_with_highest_class(df_obj = df_sub)\n    print(df_sub.info())\n    return df_sub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T19:16:04.997742Z","iopub.execute_input":"2024-11-07T19:16:04.998111Z","iopub.status.idle":"2024-11-07T19:16:05.006716Z","shell.execute_reply.started":"2024-11-07T19:16:04.998070Z","shell.execute_reply":"2024-11-07T19:16:05.005897Z"}},"outputs":[],"execution_count":9},{"id":"7f610049-92c1-41fd-8042-44373f308555","cell_type":"code","source":"if test_c_name == \"c1\":\n    df_c1 = do_preprocessing(test_c_name, category_dict[test_c_name])\nelif test_c_name == \"c2\":\n    df_c2 = do_preprocessing(test_c_name, category_dict[test_c_name])\nelif test_c_name == \"c3\":\n    df_c3 = do_preprocessing(test_c_name, category_dict[test_c_name])\nelif test_c_name == \"c4\":\n    df_c4 = do_preprocessing(test_c_name, category_dict[test_c_name])\nelif test_c_name == \"c5\":\n    df_c5 = do_preprocessing(test_c_name, category_dict[test_c_name])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T19:16:05.007923Z","iopub.execute_input":"2024-11-07T19:16:05.008233Z","iopub.status.idle":"2024-11-07T19:16:05.094574Z","shell.execute_reply.started":"2024-11-07T19:16:05.008202Z","shell.execute_reply":"2024-11-07T19:16:05.093704Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 6822 entries, 25613 to 32434\nData columns (total 13 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        6822 non-null   int64 \n 1   Category  6822 non-null   object\n 2   len       6822 non-null   int64 \n 3   attr_1    6629 non-null   object\n 4   attr_2    3231 non-null   object\n 5   attr_3    3400 non-null   object\n 6   attr_4    6431 non-null   object\n 7   attr_5    3266 non-null   object\n 8   attr_6    3848 non-null   object\n 9   attr_7    3843 non-null   object\n 10  attr_8    6702 non-null   object\n 11  attr_9    6691 non-null   object\n 12  attr_10   0 non-null      object\ndtypes: int64(2), object(11)\nmemory usage: 746.2+ KB\nNone\n<class 'pandas.core.frame.DataFrame'>\nIndex: 6822 entries, 25613 to 32434\nData columns (total 12 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        6822 non-null   int64 \n 1   Category  6822 non-null   object\n 2   len       6822 non-null   int64 \n 3   attr_1    6822 non-null   object\n 4   attr_2    6822 non-null   object\n 5   attr_3    6822 non-null   object\n 6   attr_4    6822 non-null   object\n 7   attr_5    6822 non-null   object\n 8   attr_6    6822 non-null   object\n 9   attr_7    6822 non-null   object\n 10  attr_8    6822 non-null   object\n 11  attr_9    6822 non-null   object\ndtypes: int64(2), object(10)\nmemory usage: 692.9+ KB\nNone\n","output_type":"stream"}],"execution_count":10},{"id":"db2513db","cell_type":"code","source":"class LabelEncoderDict:\n    def __init__(self):\n        self.encoders = {}\n    def fit(self, df, columns):\n        \"\"\"Fit label encoders for each column\"\"\"\n        for col in columns:\n            le = LabelEncoder()\n            # Include NaN as a unique label by appending it to valid labels\n            valid_labels = df[col].dropna().unique().tolist()\n            # valid_labels.append('NaN')  # Assign a label for NaN\n            le.fit(valid_labels)\n            self.encoders[col] = le\n    def transform(self, df, columns):\n        \"\"\"Transform labels using fitted encoders\"\"\"\n        encoded = np.zeros((len(df), len(columns)))\n        for i, col in enumerate(columns):\n            series = df[col].copy()\n            # Replace NaNs with the string 'NaN' so they can be encoded\n            # series = series.fillna('NaN')\n            encoded[:, i] = self.encoders[col].transform(series)\n        return encoded\n    def get_num_classes(self, column):\n        \"\"\"Get number of classes for a specific column\"\"\"\n        return len(self.encoders[column].classes_)\n        \nclass MultiLabelImageDataset(Dataset):\n    def __init__(self, df, image_dir, transform_basic=None, transform_augmented=None, attr_columns=10,do_transform=True):\n        self.df = df\n        self.image_dir = image_dir\n        self.transform_basic = transform_basic  # Basic transform without augmentation\n        self.transform_augmented = transform_augmented  # Augmented transform with augmentation\n        self.attr_columns = attr_columns\n        self.do_transform = do_transform\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        # Get image path\n        img_name = str(self.df.iloc[idx]['id']).zfill(6)\n        img_path = os.path.join(self.image_dir, f\"{img_name}.jpg\")\n        # Load image\n        try:\n            image = Image.open(img_path).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image {img_path}: {e}\")\n            image = Image.new('RGB', (512,512))\n        # Apply random probability to choose between augmentation or notF\n        if (random.random() > 0.5) and self.do_transform:\n            if self.transform_basic:\n                image = self.transform_basic(image)\n        else:\n            if self.transform_augmented:\n                image = self.transform_augmented(image)\n        # Ensure labels are integers and convert to tensor\n        labels = torch.tensor(self.df.iloc[idx][self.attr_columns].astype(int).values, dtype=torch.long)\n        return image, labels\n        \ndef prepare_data(df, image_dir, batch_size=32, test_size=0.2, num_attr_columns=10):\n    \"\"\"\n    Prepare data loaders and label encoders\n    \"\"\"\n    # Define attribute columns\n    attr_columns = [f'attr_{i}' for i in range(1, num_attr_columns+1)]\n    # Create and fit label encoders\n    label_encoders = LabelEncoderDict()\n    label_encoders.fit(df, attr_columns)\n    # Transform labels\n    encoded_labels = label_encoders.transform(df, attr_columns)\n    df_encoded = df.copy()\n    for i, col in enumerate(attr_columns):\n        df_encoded[col] = encoded_labels[:, i]\n    # Split data\n    train_df, val_df = train_test_split(df_encoded, test_size=test_size, random_state=42)\n    # Define transforms\n    transform = transforms.Compose([\n        transforms.Resize((512,512)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                           std=[0.229, 0.224, 0.225])\n    ])\n    transform_augmented = transforms.Compose([\n    transforms.Resize((512,512)), \n    transforms.RandomRotation(degrees=15),  # Rotate by up to 15 degrees\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Adjust color properties\n    transforms.RandomResizedCrop(size=(512, 512), scale=(0.8, 1.0)),  # Randomly crop and resize\n    transforms.RandomPerspective(distortion_scale=0.1, p=0.5),  # Apply perspective distortion\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),  # Affine transformations\n    transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),  # Apply random Gaussian blur\n    transforms.ToTensor(),  # Convert image to tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize based on ImageNet statistics\n    ])\n    # Create datasets\n    train_dataset = MultiLabelImageDataset(\n        train_df,\n        image_dir,\n        transform_basic=transform,\n        transform_augmented = transform_augmented,\n        attr_columns=attr_columns\n    )\n    val_dataset = MultiLabelImageDataset(\n        val_df,\n        image_dir,\n        transform_basic=transform,\n        transform_augmented = transform_augmented,\n        attr_columns=attr_columns,\n        do_transform = False\n    )\n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    # Get number of classes for each attribute\n    num_classes_per_attr = [label_encoders.get_num_classes(col) for col in attr_columns]\n    return train_loader, val_loader, label_encoders, num_classes_per_attr\n\nclass MultiLabelCELoss(nn.Module):\n    def __init__(self):\n        super(MultiLabelCELoss, self).__init__()\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.05)\n    def forward(self, outputs, targets):\n        # outputs is a list of predictions for each label\n        # targets is a tensor of shape (batch_size, num_labels)\n        loss = 0\n        for i, output in enumerate(outputs):\n            loss += self.criterion(output, targets[:, i])\n        return loss / len(outputs)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T19:16:05.095956Z","iopub.execute_input":"2024-11-07T19:16:05.096265Z","iopub.status.idle":"2024-11-07T19:16:05.121632Z","shell.execute_reply.started":"2024-11-07T19:16:05.096231Z","shell.execute_reply":"2024-11-07T19:16:05.120548Z"},"papermill":{"duration":0.033943,"end_time":"2024-10-28T19:37:31.860835","exception":false,"start_time":"2024-10-28T19:37:31.826892","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":11},{"id":"4f1d6378","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom tqdm import tqdm \n\nfrom transformers import ConvNextModel\nclass MultiLabelClassifier(nn.Module):\n    def __init__(self, num_classes_per_attr):\n        super(MultiLabelClassifier, self).__init__()\n        # Use ConvNeXt-Base with unfrozen backbone\n        self.backbone = ConvNextModel.from_pretrained(\"facebook/convnext-base-384-22k-1k\")\n        backbone_features = self.backbone.config.hidden_sizes[-1]  # 1024 for base model\n        # Modified feature processing without fixed dimensions\n        self.feature_processor = nn.Sequential(\n            nn.Conv2d(backbone_features, 1024, kernel_size=1),\n            nn.GELU(),\n            nn.Dropout(0.1)\n        )\n        # Complex classifier heads for each attribute\n        self.classifier_heads = nn.ModuleList()\n        for num_classes in num_classes_per_attr:\n            classifier_head = nn.Sequential(\n                # First branch - Spatial attention\n                nn.Sequential(\n                    nn.Conv2d(1024, 512, kernel_size=3, padding=1, groups=32),\n                    nn.GELU(),\n                    nn.Conv2d(512, 512, kernel_size=3, padding=1, groups=32),\n                    nn.GELU(),\n                ),\n                # Second branch - Channel attention (SE-like module)\n                nn.Sequential(\n                    nn.AdaptiveAvgPool2d(1),\n                    nn.Flatten(),\n                    nn.Linear(512, 128),\n                    nn.GELU(),\n                    nn.Linear(128, 512),\n                    nn.Sigmoid(),\n                ),\n                # Combine branches and final classification\n                nn.Sequential(\n                    nn.AdaptiveAvgPool2d(1),\n                    nn.Flatten(),\n                    nn.Linear(512, 1024),\n                    nn.LayerNorm(1024),\n                    nn.GELU(),\n                    nn.Dropout(0.2),\n                    nn.Linear(1024, 512),\n                    nn.LayerNorm(512),\n                    nn.Sigmoid(),\n                    nn.Dropout(0.1),\n                    nn.Linear(512, num_classes)\n                )\n            )\n            self.classifier_heads.append(classifier_head)\n  \n    def forward(self, x):\n        # Extract features from ConvNeXt backbone\n        features = self.backbone(x).last_hidden_state\n        # Process features\n        processed_features = self.feature_processor(features)\n        outputs = []\n        for classifier_head in self.classifier_heads:\n            # Spatial attention branch\n            spatial_features = classifier_head[0](processed_features)\n            # Channel attention branch\n            channel_attention = classifier_head[1](spatial_features)\n            channel_attention = channel_attention.view(-1, 512, 1, 1)\n            # Apply channel attention and get final output\n            attended_features = spatial_features * channel_attention\n            output = classifier_head[2](attended_features)\n            outputs.append(output)\n        return outputs\n        \nclass MultiLabelCELoss(nn.Module):\n    def __init__(self):\n        super(MultiLabelCELoss, self).__init__()\n        self.criterion = nn.CrossEntropyLoss()\n    def forward(self, outputs, targets):\n        loss = 0\n        for i, output in enumerate(outputs):\n            loss += self.criterion(output, targets[:, i])\n        return loss / len(outputs)\n\n\nfrom tqdm import tqdm  # Import tqdm for progress tracking\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch\nfrom tqdm import tqdm\nimport torch.nn as nn\n\n\ndef train_model(model, train_loader, val_loader, num_epochs, num_classes_per_attr, model_type):\n    model = torch.nn.DataParallel(model)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    criterion = MultiLabelCELoss()  # Assuming you have this loss function\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n    # Reduce learning rate on plateau\n    lr_scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n    # Early stopping params\n    early_stopping_patience = 4\n    early_stopping_counter = 0\n    best_val_avg_acc = 0.0\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        correct_predictions = [0] * len(num_classes_per_attr)\n        total_predictions = 0\n        overall_correct = 0\n        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\", unit=\"batch\") as t:\n            for images, labels in t:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                train_loss += loss.item()\n                t.set_postfix(loss=loss.item())\n                all_labels_match = torch.ones(labels.size(0), dtype=torch.bool, device=device)\n                for i, output in enumerate(outputs):\n                    _, predicted = torch.max(output, 1)\n                    correct_predictions[i] += (predicted == labels[:, i]).sum().item()\n                    all_labels_match &= (predicted == labels[:, i])\n                overall_correct += all_labels_match.sum().item()\n                total_predictions += labels.size(0)\n        # Validation phase\n        model.eval()\n        val_loss = 0\n        val_correct_predictions = [0] * len(num_classes_per_attr)\n        val_total_predictions = 0\n        val_overall_correct = 0\n        with tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\", unit=\"batch\") as v:\n            with torch.no_grad():\n                for images, labels in v:\n                    images, labels = images.to(device), labels.to(device)\n                    outputs = model(images)\n                    loss = criterion(outputs, labels)\n                    val_loss += loss.item()\n                    v.set_postfix(loss=loss.item())\n                    all_labels_match_val = torch.ones(labels.size(0), dtype=torch.bool, device=device)\n                    for i, output in enumerate(outputs):\n                        _, predicted = torch.max(output, 1)\n                        val_correct_predictions[i] += (predicted == labels[:, i]).sum().item()\n                        all_labels_match_val &= (predicted == labels[:, i])\n                    val_overall_correct += all_labels_match_val.sum().item()\n                    val_total_predictions += labels.size(0)\n        # Print results\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n        print(f'Validation Loss: {val_loss/len(val_loader):.4f}')\n        avg_acc = 0\n        for i in range(len(num_classes_per_attr)):\n            train_acc = 100 * correct_predictions[i] / total_predictions\n            val_acc = 100 * val_correct_predictions[i] / val_total_predictions\n            avg_acc += val_acc\n            print(f'Attribute {i+1} - Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n        avg_acc = avg_acc/(len(num_classes_per_attr))\n        overall_train_acc = 100 * overall_correct / total_predictions\n        overall_val_acc = 100 * val_overall_correct / val_total_predictions\n        print(f'Overall Train Accuracy: {overall_train_acc:.2f}%')\n        print(f'Overall Validation Accuracy: {overall_val_acc:.2f}%')\n        # Early stopping logic based on validation overall accuracy\n        if avg_acc >= best_val_avg_acc:\n            best_val_avg_acc = avg_acc\n            torch.save(model.module.state_dict(), f'best_model_{model_type}.pth')  # Save the best model\n            early_stopping_counter = 0  # Reset early stopping counter\n        else:\n            early_stopping_counter += 1\n        # ReduceLROnPlateau \n        lr_scheduler.step(overall_val_acc)\n        # Check early stopping condition\n        if early_stopping_counter >= early_stopping_patience:\n            print(\"Early stopping triggered\")\n    torch.save(model.module.state_dict(), f'best_model_end_{model_type}.pth')","metadata":{"papermill":{"duration":1.779751,"end_time":"2024-10-28T19:37:33.646942","exception":false,"start_time":"2024-10-28T19:37:31.867191","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T19:16:05.123374Z","iopub.execute_input":"2024-11-07T19:16:05.123684Z","iopub.status.idle":"2024-11-07T19:16:06.686188Z","shell.execute_reply.started":"2024-11-07T19:16:05.123649Z","shell.execute_reply":"2024-11-07T19:16:06.685075Z"}},"outputs":[],"execution_count":12},{"id":"51c2d061","cell_type":"code","source":"import pickle\nimport gc\n\ndef main(test_c_name):\n    # Set image directory\n    image_dir = f'{input_path}/train_images'\n    # Initialize device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    # Define a function to train each model sequentially\n    def train_single_model(data, num_attr_columns, model_type):\n        # Prepare data\n        print(f\"Preparing data for {model_type}\")\n        train_loader, val_loader, label_encoders, num_classes_per_attr = prepare_data(data, image_dir, batch_size=16, num_attr_columns=num_attr_columns)\n        world_size = torch.cuda.device_count()\n        # Initialize the model\n        print(f\"Initializing model {model_type}\")\n        model = MultiLabelClassifier(num_classes_per_attr).to(device)\n        # Define loss and optimizer\n        criterion = MultiLabelCELoss()\n        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n        # Train the model\n        print(f\"Training model {model_type}\")\n        train_model(model, train_loader, val_loader, num_epochs=NUM_EPOCH, num_classes_per_attr=num_classes_per_attr, model_type=model_type)\n        # Save label encoders\n        with open(f'label_encoders_{model_type}.pkl', 'wb') as f:\n            pickle.dump(label_encoders, f)\n        print(\"---------------------------------------------\")\n        print(f\"number of classes for {model_type} is {num_classes_per_attr}\")\n        print(\"---------------------------------------------\")\n        # Free up memory\n        del model, train_loader, val_loader, label_encoders, num_classes_per_attr\n        torch.cuda.empty_cache()\n        gc.collect()\n\n\n    if test_c_name == \"c1\":\n        train_single_model(df_c1, num_attr_columns=5, model_type=test_c_name)\n    elif test_c_name == \"c2\":\n        train_single_model(df_c2, num_attr_columns=10, model_type=test_c_name)\n    elif test_c_name == \"c3\":\n        train_single_model(df_c3, num_attr_columns=9, model_type=test_c_name)\n    elif test_c_name == \"c4\":\n        train_single_model(df_c4, num_attr_columns=8, model_type=test_c_name)\n    elif test_c_name == \"c5\":\n        train_single_model(df_c5, num_attr_columns=10, model_type=test_c_name)\n    else:\n        print(\"Please do check the name of category, something is wrong.\")","metadata":{"papermill":{"duration":0.018854,"end_time":"2024-10-28T19:37:33.672473","exception":false,"start_time":"2024-10-28T19:37:33.653619","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2024-11-07T19:16:06.687583Z","iopub.execute_input":"2024-11-07T19:16:06.688268Z","iopub.status.idle":"2024-11-07T19:16:06.701915Z","shell.execute_reply.started":"2024-11-07T19:16:06.688221Z","shell.execute_reply":"2024-11-07T19:16:06.700844Z"}},"outputs":[],"execution_count":13},{"id":"4bb73218","cell_type":"code","source":"main(test_c_name)","metadata":{"execution":{"iopub.status.busy":"2024-11-07T19:16:06.705623Z","iopub.execute_input":"2024-11-07T19:16:06.705956Z"},"papermill":{"duration":38822.857619,"end_time":"2024-10-29T06:24:36.536496","exception":false,"start_time":"2024-10-28T19:37:33.678877","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Preparing data for c3\nInitializing model c3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb197c2e575d492c8c22da7647ce5292"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/354M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"690836ebef14466cbca5d6462b68eaec"}},"metadata":{}},{"name":"stdout","text":"Training model c3\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/40 [Training]:   0%|          | 1/342 [00:04<23:51,  4.20s/batch, loss=1.01]","output_type":"stream"}],"execution_count":null},{"id":"20126b64","cell_type":"code","source":"test_df_semi = df_test[df_test['Category'] == category_dict[test_c_name]]\ntest_df_semi","metadata":{"papermill":{"duration":3.058774,"end_time":"2024-10-29T06:24:42.651793","exception":false,"start_time":"2024-10-29T06:24:39.593019","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"2810067c","cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport pickle\nfrom tqdm import tqdm\nimport time\n\ndef load_model(model_path, num_classes_per_attr, device):\n    # Initialize the model architecture and load the saved weights\n    model = MultiLabelClassifier(num_classes_per_attr)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model = torch.nn.DataParallel(model)\n    model.to(device)\n    model.eval()\n    return model\n    \ndef load_label_encoders(encoder_path):\n    with open(encoder_path, 'rb') as f:\n        encoders = pickle.load(f)\n    return encoders\n\ndef preprocess_image(image_path, image_size=(512,512)):\n    # Define image transformations (same as used during training)\n    transform = transforms.Compose([\n        transforms.Resize((512,512)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ])\n    # Open image and apply transformations\n    image = Image.open(image_path).convert('RGB')\n    image = transform(image)\n    image = image.unsqueeze(0)  # Add batch dimension\n    return image\n    \ndef inference(image, model, label_encoders):\n    # Perform inference\n    with torch.no_grad():\n        outputs = model(image)\n    # Decode predictions\n    predicted_labels = []\n    for i, output in enumerate(outputs):\n        _, predicted = torch.max(output, 1)\n        # Attribute name should match the encoder dictionary keys like 'attr_1', 'attr_2', etc.\n        attr_name = f'attr_{i + 1}'\n        if attr_name in label_encoders.encoders:\n            decoded_label = label_encoders.encoders[attr_name].inverse_transform([predicted.item()])[0]\n            predicted_labels.append(decoded_label)\n        else:\n            raise KeyError(f\"Encoder for {attr_name} not found in the loaded label encoders.\")\n    return predicted_labels\n    \n# Load the model and encoders once\nmodel_path = f\"best_model_{test_c_name}.pth\"\nencoder_path = f\"{working_path}/label_encoders_{test_c_name}.pkl\"\nnum_classes_per_attr = NUM_OF_SEMI_CLASSES_OF_COLUMNS\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = load_model(model_path, num_classes_per_attr, device)\nlabel_encoders = load_label_encoders(encoder_path)\nimage_dir = f\"{input_path}/test_images\"\npreds = {f'attr_{i}': [] for i in range(1, 10)}\nt1 = time.time()\nfor val in tqdm(test_df_semi['id'], desc='Processing Images', total=len(test_df_semi)):\n    image_path = f\"{image_dir}/{str(val).zfill(6)}.jpg\"\n    image = preprocess_image(image_path).to(device)  # Preprocess and send image to device\n    predictions = inference(image, model, label_encoders)  # Use the already loaded model and encoders\n    for i in range(1, 10):\n        preds[f'attr_{i}'].append(predictions[i-1])\nprint(f'Time taken to process images is {time.time() - t1} seconds, which is {len(test_df_semi) / (time.time() - t1)} images per second')","metadata":{"papermill":{"duration":283.157418,"end_time":"2024-10-29T06:29:28.781322","exception":false,"start_time":"2024-10-29T06:24:45.623904","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"e6140fd5","cell_type":"code","source":"for i in range(1,10):\n    test_df_semi[f'attr_{i}'] = preds[f'attr_{i}']\ntest_df_semi    ","metadata":{"papermill":{"duration":3.23632,"end_time":"2024-10-29T06:29:35.338740","exception":false,"start_time":"2024-10-29T06:29:32.102420","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"de70297f","cell_type":"code","source":"test_df_semi.to_csv(f'test_validation_df_{test_c_name}.csv',index=False)","metadata":{"papermill":{"duration":3.330766,"end_time":"2024-10-29T06:29:41.943664","exception":false,"start_time":"2024-10-29T06:29:38.612898","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"8c1480ff","cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport pickle\nfrom tqdm import tqdm\nimport time\n\ndef load_model(model_path, num_classes_per_attr, device):\n    # Initialize the model architecture and load the saved weights\n    model = MultiLabelClassifier(num_classes_per_attr)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model = torch.nn.DataParallel(model)\n    model.to(device)\n    model.eval()\n    return model\n    \ndef load_label_encoders(encoder_path):\n    with open(encoder_path, 'rb') as f:\n        encoders = pickle.load(f)\n    return encoders\ndef preprocess_image(image_path, image_size=(512,512)):\n    # Define image transformations (same as used during training)\n    transform = transforms.Compose([\n        transforms.Resize((512,512)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n    ])\n    # Open image and apply transformations\n    image = Image.open(image_path).convert('RGB')\n    image = transform(image)\n    image = image.unsqueeze(0)  # Add batch dimension\n    return image\n    \ndef inference(image, model, label_encoders):\n    # Perform inference\n    with torch.no_grad():\n        outputs = model(image)\n    # Decode predictions\n    predicted_labels = []\n    for i, output in enumerate(outputs):\n        _, predicted = torch.max(output, 1)\n        # Attribute name should match the encoder dictionary keys like 'attr_1', 'attr_2', etc.\n        attr_name = f'attr_{i + 1}'\n        if attr_name in label_encoders.encoders:\n            decoded_label = label_encoders.encoders[attr_name].inverse_transform([predicted.item()])[0]\n            predicted_labels.append(decoded_label)\n        else:\n            raise KeyError(f\"Encoder for {attr_name} not found in the loaded label encoders.\")\n    return predicted_labels\n# Load the model and encoders once\nmodel_path = f\"best_model_end_{test_c_name}.pth\"\nencoder_path = f\"{working_path}/label_encoders_{test_c_name}.pkl\"\nnum_classes_per_attr = NUM_OF_SEMI_CLASSES_OF_COLUMNS\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = load_model(model_path, num_classes_per_attr, device)\nlabel_encoders = load_label_encoders(encoder_path)\nimage_dir = f\"{input_path}/test_images\"\npreds = {f'attr_{i}': [] for i in range(1, 10)}\nt1 = time.time()\nfor val in tqdm(test_df_semi['id'], desc='Processing Images', total=len(test_df_semi)):\n    image_path = f\"{image_dir}/{str(val).zfill(6)}.jpg\"\n    image = preprocess_image(image_path).to(device)  # Preprocess and send image to device\n    predictions = inference(image, model, label_encoders)  # Use the already loaded model and encoders\n    for i in range(1, 10):\n        preds[f'attr_{i}'].append(predictions[i-1])\nprint(f'Time taken to process images is {time.time() - t1} seconds, which is {len(test_df_semi) / (time.time() - t1)} images per second')","metadata":{"papermill":{"duration":259.779973,"end_time":"2024-10-29T06:34:05.000056","exception":false,"start_time":"2024-10-29T06:29:45.220083","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"4f735274","cell_type":"code","source":"for i in range(1,10):\n    test_df_semi[f'attr_{i}'] = preds[f'attr_{i}']\ntest_df_semi    ","metadata":{"papermill":{"duration":3.452652,"end_time":"2024-10-29T06:34:11.860869","exception":false,"start_time":"2024-10-29T06:34:08.408217","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"0fd48743","cell_type":"code","source":"test_df_semi.to_csv(f'test_df_semi_{test_c_name}.csv',index=False)","metadata":{"papermill":{"duration":3.421389,"end_time":"2024-10-29T06:34:18.725784","exception":false,"start_time":"2024-10-29T06:34:15.304395","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"d8d05208","cell_type":"code","source":"","metadata":{"papermill":{"duration":3.212267,"end_time":"2024-10-29T06:34:32.374662","exception":false,"start_time":"2024-10-29T06:34:29.162395","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"id":"202abbb0-0d88-4d28-a8d0-0b39b944590f","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dbf1d55d-f9c4-4b6f-b2df-e4a43ebe88ea","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}